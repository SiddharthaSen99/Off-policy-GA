{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "max_episodes = 1000          # max training episodes\n",
    "max_timesteps = 250          # max timesteps in one episode\n",
    "gamma = 0.01                # discount factor\n",
    "epsilon = 0.2                #need to change it to max(advantage)\n",
    "dkl=1                       #need to change it to KL divergence between old and new policies\n",
    "Q_r=[]\n",
    "Q_r1=[]\n",
    "a=[]\n",
    "Q=np.zeros((max_timesteps,2))\n",
    "mutation_power = 0.02#hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "#print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAI(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                        nn.Linear(4,128, bias=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128,2, bias=True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )\n",
    "\n",
    "                \n",
    "        def forward(self, inputs):\n",
    "            x = self.fc(inputs)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \n",
    "        # nn.Conv2d weights are of shape [16, 1, 3, 3] i.e. # number of filters, 1, stride, stride\n",
    "        # nn.Conv2d bias is of shape [16] i.e. # number of filters\n",
    "        \n",
    "        # nn.Linear weights are of shape [32, 24336] i.e. # number of input features, number of output features\n",
    "        # nn.Linear bias is of shape [32] i.e. # number of output features\n",
    "        \n",
    "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavioural_policy(agents):\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(agent1,agent2):\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_agents(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        \n",
    "        agent = CartPoleAI()\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "        \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #print(\"Enter\")\n",
    "    for agent in agents:#There is only one agent in the list. But it is necessary to pass the agents in the form of lists to make it iterable and work on it\n",
    "        agent.eval()\n",
    "        #print(\"HELLLO!!!!!!\")\n",
    "        observation = env.reset()\n",
    "        r=0\n",
    "        a.clear()\n",
    "        Q_r1.clear()\n",
    "        Q_r.clear()\n",
    "        for i in range(max_timesteps):\n",
    "            \n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            #print(output_probabilities)\n",
    "            Q[i][0]=output_probabilities[0]\n",
    "            Q[i][1]=output_probabilities[1]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            a.append(action)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            r+=reward\n",
    "            Q_r.append(r)\n",
    "            observation = new_observation\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        reward_agents.append(r)\n",
    "        #reward_agents.append(s)\n",
    "        d_r=0\n",
    "        #print(\"Non-discounted Reward:::\",Q_r)\n",
    "        for rew in reversed(Q_r):\n",
    "            if rew==Q_r[len(Q_r)-1]:\n",
    "                d_r=0\n",
    "            else:\n",
    "                d_r=rew+gamma*d_r\n",
    "            Q_r1.insert(0,d_r)\n",
    "            \n",
    "        \n",
    "    #print(\"Exit\")\n",
    "    #Q=Q[~np.all(Q==0,axis=1)]\n",
    "    #print(\"Reward:::\",reward_agents)\n",
    "    #print(\"Probabilities:::\",Q)\n",
    "    #print(\"Actions:::\",a)\n",
    "    #print(\"Discounted Reward,i.e,Q:::\",Q_r1)\n",
    "    \n",
    "    \n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_target_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #print(\"Enter\")\n",
    "    for agent in agents:#There is only one agent in the list. But it is necessary to pass the agents in the form of lists to make it iterable and work on it\n",
    "        agent.eval()\n",
    "        #print(\"HELLLO!!!!!!\")\n",
    "        observation = env.reset()\n",
    "        r=0\n",
    "        s=0\n",
    "        i=0\n",
    "        #print(\"SIZE OF ACTIONS:\",len(a),\"SIZE OF PROBABILITIES:\",Q.shape,\"SIZE OF DISCOUNTED REWARDS:\",len(Q_r1))\n",
    "        for i in range(len(Q_r1)):\n",
    "            \n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            L_pi=(output_probabilities[a[i]]/Q[i][a[i]])*Q_r1[i]\n",
    "            #print(\"L_pi\",L_pi)\n",
    "            #surr1= L_pi-(4*max(Q_r1)*gamma*dkl/np.square(1-gamma))\n",
    "            \n",
    "            #print(output_probabilities)\n",
    "            s+=L_pi\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            #r+=(gamma**i)*reward\n",
    "            \n",
    "            #s=s+1\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "        #print(\"S:\",s)\n",
    "        #print(\"Max Q_r1:\",max(Q_r1))\n",
    "        surr1= s-(4*max(Q_r1)*gamma*dkl/np.square(1-gamma))\n",
    "        #print(\"Surr1:\",surr1)\n",
    "        reward_agents.append(surr1)        \n",
    "        #reward_agents.append(s)\n",
    "    #print(\"Exit\")\n",
    "    #print(\"REWARD AGENTS\",reward_agents)\n",
    "    #print(reward_agents)\n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_target_agents([agent])[0]\n",
    "    return score/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_agents([agent])[0]\n",
    "    return score/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    avg_score = []\n",
    "    for agent in agents:\n",
    "        if agent==agents[0]:\n",
    "            avg_score.append(return_average_score(agent,runs))\n",
    "        else:\n",
    "            avg_score.append(alternate_average_score(agent,runs))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "    \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            \n",
    "                            param[i0][i1][i2][i3]+= mutation_power * np.random.rand()\n",
    "                                \n",
    "                                    \n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    \n",
    "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
    "    for i in range(len(agents)-1):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    #now add one elite\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index=len(children_agents)-1 #it is the last one\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    \n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if(elite_index is not None):\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation  0  | Mean rewards:  101.20061401856285  | Mean of top 5:  123.85526172014113\n",
      "The minimum reward is earned is  21.333333333333332 by the  1 th agent\n",
      "Top  20  scores [ 99 100 262 391 199 251 430 392 328 210 259 357 419 431 208 445 256 428\n",
      " 173  93]\n",
      "Rewards for top:  [126.29910774654284, 124.94740577349404, 123.96950116851913, 122.17889247839189, 121.88140143375772, 121.62775478418594, 120.90816661035105, 120.33193892192031, 120.15725181048526, 119.7651143559064, 119.38852866832728, 118.96066145872396, 118.68341292233616, 117.5878297205361, 117.50108208405909, 117.45517676038361, 117.34457011163074, 117.3027826556703, 117.20376024540617, 117.13261450803752]\n",
      "Score for elite i  99  is  16.4\n",
      "Score for elite i  100  is  15.2\n",
      "Score for elite i  262  is  20.4\n",
      "Score for elite i  391  is  23.8\n",
      "Score for elite i  199  is  22.4\n",
      "Score for elite i  251  is  22.2\n",
      "Score for elite i  430  is  20.6\n",
      "Score for elite i  392  is  17.4\n",
      "Score for elite i  328  is  15.2\n",
      "Score for elite i  210  is  21.8\n",
      "Elite selected with index  391  and score 23.8\n",
      "\n",
      "\n",
      "Generation  1  | Mean rewards:  76.15576697660492  | Mean of top 5:  82.64342657949074\n",
      "The minimum reward is earned is  12.0 by the  1 th agent\n",
      "Top  20  scores [280 232 448 104 352 107 428 156 226 486 322 419 307 490 248 436 383 348\n",
      "  27 210]\n",
      "Rewards for top:  [83.46381733245406, 82.59844252019825, 82.53193568959165, 82.34077401316881, 82.28216334204089, 82.17591975545116, 82.00316543260948, 81.9513317165668, 81.67219640611442, 81.66398667557549, 81.64499083586539, 81.50119364967289, 81.45462176262599, 81.39656702053058, 81.37523951958765, 81.35834157789331, 81.25350447063155, 81.2402829385496, 81.2071090401924, 81.16294444911027]\n",
      "Score for elite i  280  is  18.8\n",
      "Score for elite i  232  is  20.6\n",
      "Score for elite i  448  is  19.8\n",
      "Score for elite i  104  is  22.4\n",
      "Score for elite i  352  is  22.8\n",
      "Score for elite i  107  is  17.6\n",
      "Score for elite i  428  is  22.2\n",
      "Score for elite i  156  is  17.6\n",
      "Score for elite i  226  is  21.0\n",
      "Score for elite i  486  is  12.8\n",
      "Score for elite i  499  is  19.8\n",
      "Elite selected with index  352  and score 22.8\n",
      "\n",
      "\n",
      "Generation  2  | Mean rewards:  62.7439545662848  | Mean of top 5:  67.75069036947475\n",
      "The minimum reward is earned is  13.0 by the  1 th agent\n",
      "Top  20  scores [338 460 176 259 392 433 485 106 484 327 115 222 440   8  93  31 436 200\n",
      " 422  78]\n",
      "Rewards for top:  [68.84796818123442, 67.63747056384203, 67.62997194693462, 67.40716474593587, 67.23087640942678, 67.213558107815, 67.133559772078, 67.04887178630754, 67.04049889575332, 67.01172054242225, 67.00326236315331, 66.96328389099934, 66.96153868817917, 66.93485736721281, 66.93078271298963, 66.91391547303827, 66.81926710898135, 66.79856708599662, 66.73665721957987, 66.72090206731251]\n",
      "Score for elite i  338  is  16.8\n",
      "Score for elite i  460  is  15.4\n",
      "Score for elite i  176  is  20.6\n",
      "Score for elite i  259  is  13.4\n",
      "Score for elite i  392  is  15.8\n",
      "Score for elite i  433  is  14.6\n",
      "Score for elite i  485  is  16.4\n",
      "Score for elite i  106  is  14.2\n",
      "Score for elite i  484  is  21.4\n",
      "Score for elite i  327  is  12.2\n",
      "Score for elite i  499  is  18.2\n",
      "Elite selected with index  484  and score 21.4\n",
      "\n",
      "\n",
      "Generation  3  | Mean rewards:  62.29953744808187  | Mean of top 5:  67.58957849379372\n",
      "The minimum reward is earned is  15.0 by the  1 th agent\n",
      "Top  20  scores [  4 375 423  61 327 200 464 405 150 177  34 462 458 380 347 165  35 488\n",
      " 163 374]\n",
      "Rewards for top:  [68.65407836579898, 67.45908076351236, 67.32450561353949, 67.277296369332, 67.2329313567858, 66.83143891759148, 66.82684335106427, 66.81610834998489, 66.64787125721261, 66.56612598818205, 66.5200774624712, 66.51269658930141, 66.4995778239633, 66.49042957273525, 66.44223192190277, 66.40604094116402, 66.40583158230153, 66.34707000748953, 66.34665450340462, 66.26557874573162]\n",
      "Score for elite i  4  is  22.6\n",
      "Score for elite i  375  is  14.0\n",
      "Score for elite i  423  is  15.2\n",
      "Score for elite i  61  is  20.2\n",
      "Score for elite i  327  is  22.4\n",
      "Score for elite i  200  is  15.0\n",
      "Score for elite i  464  is  16.2\n",
      "Score for elite i  405  is  17.6\n",
      "Score for elite i  150  is  14.6\n",
      "Score for elite i  177  is  16.2\n",
      "Score for elite i  499  is  18.4\n",
      "Elite selected with index  4  and score 22.6\n",
      "\n",
      "\n",
      "Generation  4  | Mean rewards:  53.72514384881639  | Mean of top 5:  57.00070745931415\n",
      "The minimum reward is earned is  11.666666666666666 by the  1 th agent\n",
      "Top  20  scores [314  71 423 197 115 248 211  49 297 237 364 373 491 343 282 171 288 172\n",
      " 231 200]\n",
      "Rewards for top:  [57.4850998648783, 57.16794239707293, 57.122614319622905, 56.69237648365006, 56.53550423134654, 56.35881899780545, 56.28754887410037, 56.276135410655286, 56.079956155671766, 56.03709843599851, 56.034914977783615, 56.03058716554855, 56.0265335050405, 56.00268364533546, 55.99760228258359, 55.99315412890345, 55.98943320318779, 55.960813231463355, 55.9342795067759, 55.8918956529236]\n",
      "Score for elite i  314  is  14.4\n",
      "Score for elite i  71  is  28.6\n",
      "Score for elite i  423  is  17.6\n",
      "Score for elite i  197  is  11.8\n",
      "Score for elite i  115  is  11.6\n",
      "Score for elite i  248  is  17.8\n",
      "Score for elite i  211  is  18.2\n",
      "Score for elite i  49  is  15.4\n",
      "Score for elite i  297  is  11.6\n",
      "Score for elite i  237  is  24.0\n",
      "Score for elite i  499  is  15.2\n",
      "Elite selected with index  71  and score 28.6\n",
      "\n",
      "\n",
      "Generation  5  | Mean rewards:  99.17880166927787  | Mean of top 5:  121.52710168549831\n",
      "The minimum reward is earned is  13.0 by the  1 th agent\n",
      "Top  20  scores [421 273 302 116 275 210 243 115 343 469 432 288 338 102 303 238 190 291\n",
      " 394 490]\n",
      "Rewards for top:  [123.81276148114817, 121.16777547161065, 121.02969501324367, 120.84973916830864, 120.77553729318045, 120.77128984033271, 120.55967004499668, 120.4932879427215, 120.48935650828453, 120.42859175447161, 120.2824332259512, 120.184918288037, 120.073306883373, 119.88272773798998, 119.79673745456712, 119.70149840152057, 119.65756546949574, 119.58376204767568, 119.57450774926765, 119.4972714230862]\n",
      "Score for elite i  421  is  18.4\n",
      "Score for elite i  273  is  17.8\n",
      "Score for elite i  302  is  21.4\n",
      "Score for elite i  116  is  16.8\n",
      "Score for elite i  275  is  15.2\n",
      "Score for elite i  210  is  14.0\n",
      "Score for elite i  243  is  21.6\n",
      "Score for elite i  115  is  22.0\n",
      "Score for elite i  343  is  14.2\n",
      "Score for elite i  469  is  25.4\n",
      "Score for elite i  499  is  20.2\n",
      "Elite selected with index  469  and score 25.4\n",
      "\n",
      "\n",
      "Generation  6  | Mean rewards:  80.44476182142937  | Mean of top 5:  91.68081563416102\n",
      "The minimum reward is earned is  13.666666666666666 by the  1 th agent\n",
      "Top  20  scores [447  18 339  46 153 142  85 402 340 483 380 257 236 444 297 399  60 282\n",
      " 345 403]\n",
      "Rewards for top:  [92.18819134828588, 91.80251755264929, 91.6698932600457, 91.50863095486243, 91.23484505496181, 91.19498987987981, 90.83548985117018, 90.82588653921137, 90.74822898171071, 90.67716304059326, 90.65322367751791, 90.55581397078743, 90.50766907133455, 90.47869571451379, 90.44543920871223, 90.38373164073231, 90.24842524649961, 90.23963010606144, 90.08271212661157, 90.03838255765952]\n",
      "Score for elite i  447  is  12.0\n",
      "Score for elite i  18  is  21.8\n",
      "Score for elite i  339  is  16.0\n",
      "Score for elite i  46  is  11.8\n",
      "Score for elite i  153  is  18.6\n",
      "Score for elite i  142  is  16.4\n",
      "Score for elite i  85  is  15.0\n",
      "Score for elite i  402  is  18.6\n",
      "Score for elite i  340  is  14.8\n",
      "Score for elite i  483  is  15.0\n",
      "Score for elite i  499  is  18.8\n",
      "Elite selected with index  18  and score 21.8\n",
      "\n",
      "\n",
      "Generation  7  | Mean rewards:  90.00777734958645  | Mean of top 5:  103.83071287054688\n",
      "The minimum reward is earned is  13.666666666666666 by the  1 th agent\n",
      "Top  20  scores [452 263 109 116 268 295 200 375  26  68 306  59 336 158 103 309 366 320\n",
      " 150  78]\n",
      "Rewards for top:  [104.22343342678528, 103.80182126470436, 103.74962066305712, 103.72152433874338, 103.65716465944426, 103.64202770416064, 103.55691170434761, 103.5043022140173, 103.50197189360506, 103.48935700091216, 103.46736606086687, 103.39255567207215, 103.25244213639898, 103.23509332585185, 103.21672705611884, 103.21403826559559, 103.21251183997738, 103.0639249439131, 103.01409976122277, 102.95727534329218]\n",
      "Score for elite i  452  is  16.6\n",
      "Score for elite i  263  is  18.2\n",
      "Score for elite i  109  is  15.8\n",
      "Score for elite i  116  is  17.6\n",
      "Score for elite i  268  is  15.8\n",
      "Score for elite i  295  is  18.4\n",
      "Score for elite i  200  is  19.8\n",
      "Score for elite i  375  is  15.4\n",
      "Score for elite i  26  is  24.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for elite i  68  is  15.6\n",
      "Score for elite i  499  is  18.8\n",
      "Elite selected with index  26  and score 24.6\n",
      "\n",
      "\n",
      "Generation  8  | Mean rewards:  32.407255492434395  | Mean of top 5:  35.63486256128729\n",
      "The minimum reward is earned is  15.666666666666666 by the  1 th agent\n",
      "Top  20  scores [271 111 224 446 428 450 344 310 429  94  86 494 322 169 351 272  72 291\n",
      " 214  61]\n",
      "Rewards for top:  [36.762060853375125, 35.67603552408111, 35.423804673197104, 35.156337713666375, 35.156074042116764, 35.12779699862045, 35.00884776820399, 34.9651842776369, 34.927322167134946, 34.86697827664968, 34.843681864900624, 34.75749706720837, 34.73563978498163, 34.72710522245965, 34.72543145076763, 34.709236237042795, 34.699543738268886, 34.69261951030205, 34.67576283521677, 34.63829366313777]\n",
      "Score for elite i  271  is  16.8\n",
      "Score for elite i  111  is  17.4\n",
      "Score for elite i  224  is  12.4\n",
      "Score for elite i  446  is  16.4\n",
      "Score for elite i  428  is  14.2\n",
      "Score for elite i  450  is  18.6\n",
      "Score for elite i  344  is  15.8\n",
      "Score for elite i  310  is  19.4\n",
      "Score for elite i  429  is  14.4\n",
      "Score for elite i  94  is  15.4\n",
      "Score for elite i  499  is  16.0\n",
      "Elite selected with index  310  and score 19.4\n",
      "\n",
      "\n",
      "Generation  9  | Mean rewards:  139.62308471608026  | Mean of top 5:  235.45486910349646\n",
      "The minimum reward is earned is  20.666666666666668 by the  1 th agent\n",
      "Top  20  scores [398 257 252 129 106 496  40 130 258 370   1  73 150 214 350 385 476 184\n",
      " 369  60]\n",
      "Rewards for top:  [236.8384459386773, 235.32167734468604, 235.2523499483948, 235.13593353269928, 234.7259387530249, 234.4858089958701, 234.4642190544433, 230.97632356531813, 227.34703377737378, 223.99968225158116, 222.28687586285915, 222.26780209267676, 220.8669012857522, 218.1337234997168, 216.7121242879216, 216.27431558406525, 215.67004777490425, 215.56458334584738, 215.3878607718058, 214.4987164045176]\n",
      "Score for elite i  398  is  21.8\n",
      "Score for elite i  257  is  17.4\n",
      "Score for elite i  252  is  19.0\n",
      "Score for elite i  129  is  21.2\n",
      "Score for elite i  106  is  13.2\n",
      "Score for elite i  496  is  16.6\n",
      "Score for elite i  40  is  19.8\n",
      "Score for elite i  130  is  12.2\n",
      "Score for elite i  258  is  13.8\n",
      "Score for elite i  370  is  26.2\n",
      "Score for elite i  499  is  20.2\n",
      "Elite selected with index  370  and score 26.2\n",
      "\n",
      "\n",
      "Generation  10  | Mean rewards:  106.7075821803741  | Mean of top 5:  139.5176999110176\n",
      "The minimum reward is earned is  16.0 by the  1 th agent\n",
      "Top  20  scores [ 99 277 160 362 292  75 191 162 230 267 379  31 152 309 306 272 483 167\n",
      "  25 186]\n",
      "Rewards for top:  [140.0561815089068, 139.9730485317187, 139.75285553122373, 139.23422300721344, 138.57219097602527, 138.55126970732033, 138.37341609279218, 138.26918046510488, 138.19222618940935, 138.15241942607307, 138.01608178183986, 137.9966790216259, 137.7789534558577, 137.7696547789628, 137.6121319693815, 137.07000019404026, 136.9203766158089, 136.8833428434193, 136.79902110372078, 136.71415538491541]\n",
      "Score for elite i  99  is  19.0\n",
      "Score for elite i  277  is  11.6\n",
      "Score for elite i  160  is  18.2\n",
      "Score for elite i  362  is  19.0\n",
      "Score for elite i  292  is  17.4\n",
      "Score for elite i  75  is  17.6\n",
      "Score for elite i  191  is  15.6\n",
      "Score for elite i  162  is  19.8\n",
      "Score for elite i  230  is  21.4\n",
      "Score for elite i  267  is  13.6\n",
      "Score for elite i  499  is  17.2\n",
      "Elite selected with index  230  and score 21.4\n",
      "\n",
      "\n",
      "Generation  11  | Mean rewards:  43.94751053674339  | Mean of top 5:  45.70203547869522\n",
      "The minimum reward is earned is  12.0 by the  1 th agent\n",
      "Top  20  scores [197 212 226 257 262  39 336 219 307 222  56 214 475 209 191 460 186  40\n",
      " 295 381]\n",
      "Rewards for top:  [45.83926248163392, 45.73861284046026, 45.690070648235285, 45.67236732629011, 45.56986409685655, 45.568476709727854, 45.566518755321134, 45.52378691508591, 45.511846459420866, 45.508036981291745, 45.49980648368693, 45.41440813970064, 45.39994703767247, 45.391614602700436, 45.38931566497721, 45.37887985979736, 45.34759724222254, 45.33049857571848, 45.31894089761544, 45.31643930407205]\n",
      "Score for elite i  197  is  13.8\n",
      "Score for elite i  212  is  16.2\n",
      "Score for elite i  226  is  17.0\n",
      "Score for elite i  257  is  19.4\n",
      "Score for elite i  262  is  14.2\n",
      "Score for elite i  39  is  15.4\n",
      "Score for elite i  336  is  12.4\n",
      "Score for elite i  219  is  12.2\n",
      "Score for elite i  307  is  12.4\n",
      "Score for elite i  222  is  17.4\n",
      "Score for elite i  499  is  15.6\n",
      "Elite selected with index  257  and score 19.4\n",
      "\n",
      "\n",
      "Generation  12  | Mean rewards:  58.059249814609416  | Mean of top 5:  63.97016428104113\n",
      "The minimum reward is earned is  14.0 by the  1 th agent\n",
      "Top  20  scores [254 156 228 401 417 296 144 369  80 220 265 311 161 141  53 219 407 151\n",
      " 262 240]\n",
      "Rewards for top:  [64.17693425842081, 64.17596125604017, 63.935956052011534, 63.84804460239864, 63.71392523633447, 63.69097578048609, 63.480040417052976, 63.455454445003134, 63.39154979113457, 63.2692220152823, 63.12333513636778, 63.030702386431564, 62.99445197245953, 62.900798150310756, 62.888849101248525, 62.844279629301774, 62.756328898272784, 62.715554592361975, 62.68163968100699, 62.55720393364158]\n",
      "Score for elite i  254  is  28.4\n",
      "Score for elite i  156  is  11.4\n",
      "Score for elite i  228  is  20.0\n",
      "Score for elite i  401  is  22.0\n",
      "Score for elite i  417  is  15.4\n",
      "Score for elite i  296  is  12.6\n",
      "Score for elite i  144  is  13.6\n",
      "Score for elite i  369  is  21.6\n",
      "Score for elite i  80  is  17.6\n",
      "Score for elite i  220  is  18.0\n",
      "Score for elite i  499  is  11.4\n",
      "Elite selected with index  254  and score 28.4\n",
      "\n",
      "\n",
      "Generation  13  | Mean rewards:  52.26775281341898  | Mean of top 5:  55.311754348412066\n",
      "The minimum reward is earned is  18.0 by the  1 th agent\n",
      "Top  20  scores [406  65 440 183 146 369 338 359 415  76   1 471 114 377 113 246 317 404\n",
      "  19 190]\n",
      "Rewards for top:  [55.97967590943909, 55.34512698771284, 55.1684482638997, 55.07523367773888, 54.990286903269826, 54.941407402301856, 54.88741715959282, 54.786094798298016, 54.78372575548047, 54.717212305378865, 54.685646551636104, 54.65049896205798, 54.6323639071986, 54.610091752694096, 54.53601559258151, 54.514288154777255, 54.47588079568566, 54.449858948751945, 54.448758820969346, 54.43539954295388]\n",
      "Score for elite i  406  is  20.8\n",
      "Score for elite i  65  is  22.4\n",
      "Score for elite i  440  is  19.0\n",
      "Score for elite i  183  is  15.4\n",
      "Score for elite i  146  is  17.2\n",
      "Score for elite i  369  is  17.0\n",
      "Score for elite i  338  is  17.0\n",
      "Score for elite i  359  is  20.4\n",
      "Score for elite i  415  is  17.0\n",
      "Score for elite i  76  is  15.6\n",
      "Score for elite i  499  is  15.8\n",
      "Elite selected with index  65  and score 22.4\n",
      "\n",
      "\n",
      "Generation  14  | Mean rewards:  99.30445424200511  | Mean of top 5:  134.54900191065641\n",
      "The minimum reward is earned is  13.666666666666666 by the  1 th agent\n",
      "Top  20  scores [392 219 189 360 197 489 217 133 279  28 283 129  73 383 196 105   8 286\n",
      " 130 125]\n",
      "Rewards for top:  [135.16704006035232, 134.7993478854096, 134.6260021525003, 134.18365705223928, 133.9689624027807, 133.34266521969093, 133.28709052450844, 133.22012970500404, 133.09409510796525, 132.9277699104171, 132.84198925647897, 132.43524872067485, 132.39041570662891, 131.94280261627785, 131.90794842125467, 131.86958616887432, 131.7470134738305, 131.60920028597775, 131.5466366156668, 131.51826618459123]\n",
      "Score for elite i  392  is  17.2\n",
      "Score for elite i  219  is  12.4\n",
      "Score for elite i  189  is  18.0\n",
      "Score for elite i  360  is  12.2\n",
      "Score for elite i  197  is  11.8\n",
      "Score for elite i  489  is  14.8\n",
      "Score for elite i  217  is  15.2\n",
      "Score for elite i  133  is  14.8\n",
      "Score for elite i  279  is  14.6\n",
      "Score for elite i  28  is  17.6\n",
      "Score for elite i  499  is  16.6\n",
      "Elite selected with index  189  and score 18.0\n",
      "\n",
      "\n",
      "Generation  15  | Mean rewards:  90.05560749074182  | Mean of top 5:  106.05898548022094\n",
      "The minimum reward is earned is  17.0 by the  1 th agent\n",
      "Top  20  scores [224 205 295 157 156 469 233 221 423 367  94 314 155  49 395 399 107  41\n",
      " 425 277]\n",
      "Rewards for top:  [106.32898033231997, 106.05461744104707, 106.00015010002123, 105.99138159198617, 105.91979793573027, 105.79652332751675, 105.77905786872621, 105.72086850394165, 105.67765880638171, 105.59087342514776, 105.51889483145676, 105.50363052353998, 105.47779824401994, 105.41806682866797, 105.40542333129638, 105.34643822183233, 105.33781983572503, 105.30809639265503, 105.25164175848755, 105.24743253461311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for elite i  224  is  14.4\n",
      "Score for elite i  205  is  16.0\n",
      "Score for elite i  295  is  17.6\n",
      "Score for elite i  157  is  16.0\n",
      "Score for elite i  156  is  22.4\n",
      "Score for elite i  469  is  23.0\n",
      "Score for elite i  233  is  17.8\n",
      "Score for elite i  221  is  13.0\n",
      "Score for elite i  423  is  13.6\n",
      "Score for elite i  367  is  20.0\n",
      "Score for elite i  499  is  28.0\n",
      "Elite selected with index  499  and score 28.0\n",
      "\n",
      "\n",
      "Generation  16  | Mean rewards:  141.26658547737026  | Mean of top 5:  271.518441152984\n",
      "The minimum reward is earned is  22.666666666666668 by the  1 th agent\n",
      "Top  20  scores [352  13 115  39 306 153 166 182  98 348 119 220 368 286  87  21 178 427\n",
      " 292 177]\n",
      "Rewards for top:  [277.7861965434696, 276.64068556809957, 272.38792355764457, 267.88678244791845, 262.89061764778785, 260.9423391434313, 258.2996103017277, 253.4989763136414, 252.16815279420265, 250.88651328082486, 250.84139116896313, 250.78340947622056, 246.06989749161062, 246.03262455562344, 245.2309492698098, 242.220972089548, 240.08943154465956, 239.66721364016522, 238.74904293761043, 237.00249917795986]\n",
      "Score for elite i  352  is  14.4\n",
      "Score for elite i  13  is  15.2\n",
      "Score for elite i  115  is  16.0\n",
      "Score for elite i  39  is  18.6\n",
      "Score for elite i  306  is  17.2\n",
      "Score for elite i  153  is  16.2\n",
      "Score for elite i  166  is  15.4\n",
      "Score for elite i  182  is  12.8\n",
      "Score for elite i  98  is  23.2\n",
      "Score for elite i  348  is  23.2\n",
      "Score for elite i  499  is  17.4\n",
      "Elite selected with index  98  and score 23.2\n",
      "\n",
      "\n",
      "Generation  17  | Mean rewards:  63.004656716949086  | Mean of top 5:  67.90113098181996\n",
      "The minimum reward is earned is  20.0 by the  1 th agent\n",
      "Top  20  scores [334 348 328 452 342 419 343 475 382 448 467 433 351 138  49 399 165 249\n",
      " 109 235]\n",
      "Rewards for top:  [68.06129971346063, 68.0419489216706, 67.86215049987386, 67.8181056098815, 67.7221501642132, 67.61552556616907, 67.55725552981234, 67.55368050194839, 67.47731752579732, 67.44612248313071, 67.35526874090388, 67.3350774484145, 67.29465413325823, 67.22414467290005, 67.21453215768695, 67.16997725470681, 67.16209079530282, 67.13010265936178, 67.09984398408126, 67.0778778243599]\n",
      "Score for elite i  334  is  19.8\n",
      "Score for elite i  348  is  18.0\n",
      "Score for elite i  328  is  13.4\n",
      "Score for elite i  452  is  16.2\n",
      "Score for elite i  342  is  15.4\n",
      "Score for elite i  419  is  15.6\n",
      "Score for elite i  343  is  15.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 1 must be a class",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-57c0c5778dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# setup an empty list for containing children agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mchildren_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_parent_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# kill all agents, and replace them with their children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-760252dbc6c4>\u001b[0m in \u001b[0;36mreturn_children\u001b[0;34m(agents, sorted_parent_indexes, elite_index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#now add one elite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0melite_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_elite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_parent_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mchildren_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melite_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0melite_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m#it is the last one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-190-9f1549e3d967>\u001b[0m in \u001b[0;36madd_elite\u001b[0;34m(agents, sorted_parent_indexes, elite_index, only_consider_top_n)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_elite_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_average_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score for elite i \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-d0bf248736d3>\u001b[0m in \u001b[0;36mreturn_average_score\u001b[0;34m(agent, runs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrun_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-4bdcf26cbebc>\u001b[0m in \u001b[0;36mrun_agents\u001b[0;34m(agents)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \"\"\"\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_actions = 2 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 500\n",
    "agents = return_random_agents(num_agents)\n",
    "\n",
    "# How many top agents to consider as parents\n",
    "top_limit = 20\n",
    "\n",
    "# run evolution until X generations\n",
    "generations = 10\n",
    "\n",
    "elite_index = None\n",
    "n=[]\n",
    "m=[]\n",
    "\n",
    "for generation in range(generations):\n",
    "\n",
    "    # return rewards of agents\n",
    "    rewards = run_agents_n_times(agents, 3) #return average of 3 runs later\n",
    "    #print(rewards)\n",
    "    #sort by rewards\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit] #reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    top_rewards = []\n",
    "    \n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "    print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "    #print(rewards)\n",
    "    print(\"The minimum reward is earned is \",min(rewards),\"by the \",rewards.index(min(rewards))+1,\"th agent\")\n",
    "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    \n",
    "    m.append(np.mean(top_rewards))\n",
    "    n.append(np.mean(rewards))\n",
    "    # setup an empty list for containing children agents\n",
    "    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "\n",
    "    # kill all agents, and replace them with their children\n",
    "    agents = children_agents\n",
    "x=np.arange(generations)\n",
    "plt.plot(x,m)\n",
    "plt.plot(x,n)\n",
    "plt.title('Improvement of Mean Rewards in increasing Generations(Training)')\n",
    "plt.ylabel('Mean Rewards for Agents : Top 5 in Blue')\n",
    "plt.xlabel('Generations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        \n",
    "        env_record = Monitor(env, './video', force=True)\n",
    "        observation = env_record.reset()\n",
    "        last_observation = observation\n",
    "        r=0\n",
    "        j=[]\n",
    "        episode_durations=[]\n",
    "        timestep=0\n",
    "        for timestep in range(1000):\n",
    "            env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env_record.step(action)\n",
    "            r=r+reward\n",
    "            j.append(r)\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        env_record.close()\n",
    "\n",
    "        print(\"Rewards: \",r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_agent(agents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
