{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "    ##Changed this part    \n",
    "    def forward(self, inputs):\n",
    "            x = self.action_layer(inputs)\n",
    "            return x\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device) \n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        surr_loss = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        #print(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        #print(\"Rewardsd Mean: \") \n",
    "        #print(rewards.mean())\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            print(\"State Values : \", state_values.detach())\n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            print(\"Advantages : \", advantages)\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            surr_loss.append(loss)\n",
    "            print(\"L Clip : \", loss.mean())\n",
    "            \n",
    "            # take gradient step\n",
    "            #self.optimizer.zero_grad()\n",
    "            #loss.mean().backward()\n",
    "            #self.optimizer.step()\n",
    "            \n",
    "                  \n",
    "        # Copy new weights into old policy:       \n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        #print(\"Surrogate Loss Mean: \",surr1.mean())\n",
    "        return surr1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_reward = 30         # stop training if avg_reward > solved_reward\n",
    "log_interval = 20           # print avg reward in the interval\n",
    "max_episodes = 500        # max training episodes\n",
    "max_timesteps = 30         # max timesteps in one episode\n",
    "n_latent_var = 64           # number of variables in hidden layer\n",
    "update_timestep = 2000      # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 1                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "random_seed = None\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_dim = 4\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_agents(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        ##Change this part\n",
    "        agent = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "         ##Commented this part   \n",
    "        #init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "        \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried to change this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    total_loss = []\n",
    "    env_name = \"LunarLander-v2\"\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    if random_seed:\n",
    "        torch.manual_seed(random_seed)\n",
    "        env.seed(random_seed)\n",
    "    \n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "    #print(lr,betas)\n",
    "    \n",
    "    # logging variables\n",
    "    running_reward = 0\n",
    "    avg_length = 0\n",
    "    timestep = 0\n",
    "    surri=0\n",
    "    s1 = []\n",
    "    s2 = 0\n",
    "    s_temp = []\n",
    "    surr_actual = []\n",
    "    \n",
    "    # training loop\n",
    "    #for agent in agents:\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        #agent.eval()\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            timestep += 1\n",
    "            \n",
    "            # Running policy_old:\n",
    "            action = ppo.policy_old.act(state, memory)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Saving reward and is_terminal:\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            # update if its time\n",
    "            if timestep % update_timestep == 0:\n",
    "                surri=ppo.update(memory)\n",
    "                print(\"returned Surrogate Loss \", surri)\n",
    "                #memory.clear_memory()\n",
    "                timestep = 0\n",
    "            \n",
    "            running_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        avg_length += t\n",
    "        \n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n",
    "            break\n",
    "            \n",
    "        # logging\n",
    "        if i_episode % log_interval == 0:\n",
    "            avg_length = int(avg_length/log_interval)\n",
    "            running_reward = int((running_reward/log_interval))\n",
    "            print(' Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
    "            running_reward = 0\n",
    "            avg_length = 0\n",
    "        s1.append(surri)\n",
    "        #surr_mean = torch.mean(torch.stack(surr_actual))    \n",
    "    s1 = [i for i in s1 if i != 0]\n",
    "    #print(torch.mean(torch.stack(s1)))\n",
    "    #print(\"\\n\")\n",
    "            \n",
    "    #print(total_loss)\n",
    "    return torch.mean(torch.stack(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    #score = 0.\n",
    "    #for i in range(runs):\n",
    "    #print(run_agents([agent]))\n",
    "    score = run_agents([agent])\n",
    "    return score#/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <--Till Now Only worked -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    avg_score = []\n",
    "    for agent in agents:\n",
    "        avg_score.append(return_average_score(agent,runs))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    \n",
    "    mutation_power = 0.02 #hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "    \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            \n",
    "                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                                \n",
    "                                    \n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    \n",
    "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
    "    for i in range(len(agents)-1):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    #now add one elite\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index=len(children_agents)-1 #it is the last one\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    \n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if(elite_index is not None):\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 20 \t avg length: 29 \t reward: -22\n",
      " Episode 40 \t avg length: 29 \t reward: -24\n",
      " Episode 60 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.0563, 0.0542, 0.0534,  ..., 0.0355, 0.0355, 0.0347])\n",
      "Advantages :  tensor([0.4568, 0.4432, 0.4988,  ..., 3.0398, 3.0652, 3.1355],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5244, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0375, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -22\n",
      " Episode 100 \t avg length: 29 \t reward: -26\n",
      " Episode 120 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.0563, 0.0542, 0.0534,  ..., 0.0476, 0.0473, 0.0479])\n",
      "Advantages :  tensor([0.7584, 0.7426, 0.8059,  ..., 3.3939, 3.5631, 3.5565],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5247, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0375, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -24\n",
      " Episode 160 \t avg length: 29 \t reward: -22\n",
      " Episode 180 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.0563, 0.0542, 0.0534,  ..., 0.0416, 0.0412, 0.0399])\n",
      "Advantages :  tensor([0.7599, 0.7445, 0.8064,  ..., 3.3722, 3.4422, 3.4762],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5251, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0379, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -24\n",
      " Episode 220 \t avg length: 29 \t reward: -21\n",
      " Episode 240 \t avg length: 29 \t reward: -19\n",
      " Episode 260 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.0563, 0.0542, 0.0534,  ..., 0.0222, 0.0202, 0.0186])\n",
      "Advantages :  tensor([0.6961, 0.6809, 0.7422,  ..., 3.1752, 3.2672, 3.3874],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5251, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0379, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -22\n",
      " Episode 300 \t avg length: 29 \t reward: -16\n",
      " Episode 320 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.0563, 0.0542, 0.0534,  ..., 0.0444, 0.0425, 0.0421])\n",
      "Advantages :  tensor([0.6446, 0.6293, 0.6909,  ..., 3.4273, 3.4098, 3.4680],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5254, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0382, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -21\n",
      " Episode 360 \t avg length: 29 \t reward: -25\n",
      " Episode 380 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.0563, 0.0542, 0.0534,  ..., 0.0452, 0.0453, 0.0450])\n",
      "Advantages :  tensor([0.6983, 0.6835, 0.7436,  ..., 3.3320, 3.3390, 3.4034],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5253, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0381, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -26\n",
      " Episode 420 \t avg length: 29 \t reward: -25\n",
      " Episode 440 \t avg length: 29 \t reward: -20\n",
      " Episode 460 \t avg length: 29 \t reward: -31\n",
      "State Values :  tensor([0.0563, 0.0542, 0.0534,  ..., 0.0491, 0.0505, 0.0525])\n",
      "Advantages :  tensor([0.7417, 0.7274, 0.7854,  ..., 3.2734, 3.3711, 3.3705],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5254, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0381, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -27\n",
      " Episode 500 \t avg length: 29 \t reward: -26\n",
      " Episode 20 \t avg length: 29 \t reward: -22\n",
      " Episode 40 \t avg length: 29 \t reward: -22\n",
      " Episode 60 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.1029, -0.1033, -0.1010,  ..., -0.1101, -0.1118, -0.1111])\n",
      "Advantages :  tensor([ 0.0097, -0.0552,  0.0273,  ...,  3.2350,  3.1918,  3.2220],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3711, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1212, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -23\n",
      " Episode 100 \t avg length: 29 \t reward: -23\n",
      " Episode 120 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([-0.1029, -0.1033, -0.1010,  ..., -0.0868, -0.0853, -0.0842])\n",
      "Advantages :  tensor([0.2911, 0.2166, 0.3115,  ..., 3.5630, 3.7006, 3.8401],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3747, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1196, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -29\n",
      " Episode 180 \t avg length: 29 \t reward: -30\n",
      "State Values :  tensor([-0.1029, -0.1033, -0.1010,  ..., -0.1219, -0.1229, -0.1238])\n",
      "Advantages :  tensor([0.4077, 0.3414, 0.4257,  ..., 3.5899, 3.6716, 3.7410],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3734, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1205, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -20\n",
      " Episode 220 \t avg length: 29 \t reward: -25\n",
      " Episode 240 \t avg length: 29 \t reward: -27\n",
      " Episode 260 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.1029, -0.1033, -0.1010,  ..., -0.1477, -0.1488, -0.1498])\n",
      "Advantages :  tensor([0.4993, 0.4326, 0.5174,  ..., 3.6269, 3.7553, 3.8901],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3741, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1199, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -24\n",
      " Episode 300 \t avg length: 29 \t reward: -24\n",
      " Episode 320 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([-0.1029, -0.1033, -0.1010,  ..., -0.0826, -0.0827, -0.0828])\n",
      "Advantages :  tensor([0.3993, 0.3328, 0.4173,  ..., 3.5985, 3.6706, 3.7240],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3743, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1201, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -26\n",
      " Episode 360 \t avg length: 29 \t reward: -18\n",
      " Episode 380 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([-0.1029, -0.1033, -0.1010,  ..., -0.1338, -0.1327, -0.1334])\n",
      "Advantages :  tensor([0.3295, 0.2639, 0.3473,  ..., 3.5415, 3.5717, 3.6497],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3749, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1202, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -26\n",
      " Episode 420 \t avg length: 29 \t reward: -25\n",
      " Episode 440 \t avg length: 29 \t reward: -20\n",
      " Episode 460 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.1029, -0.1033, -0.1010,  ..., -0.1597, -0.1600, -0.1572])\n",
      "Advantages :  tensor([0.2861, 0.2228, 0.3032,  ..., 3.4227, 3.4798, 3.4663],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3760, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1200, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -19\n",
      " Episode 500 \t avg length: 29 \t reward: -18\n",
      " Episode 20 \t avg length: 29 \t reward: -26\n",
      " Episode 40 \t avg length: 29 \t reward: -22\n",
      " Episode 60 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.1974, 0.1973, 0.1952,  ..., 0.2136, 0.2121, 0.2131])\n",
      "Advantages :  tensor([-0.8687, -0.8809, -0.9263,  ...,  2.7580,  2.7411,  2.8264],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6935, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1916, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -26\n",
      " Episode 100 \t avg length: 29 \t reward: -23\n",
      " Episode 120 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.1974, 0.1973, 0.1952,  ..., 0.1956, 0.1954, 0.1955])\n",
      "Advantages :  tensor([-0.9303, -0.9427, -0.9894,  ...,  2.5996,  2.7068,  2.8677],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6956, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1912, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -27\n",
      " Episode 160 \t avg length: 29 \t reward: -19\n",
      " Episode 180 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([0.1974, 0.1973, 0.1952,  ..., 0.1732, 0.1745, 0.1746])\n",
      "Advantages :  tensor([-1.0384, -1.0526, -1.1063,  ...,  2.9713,  3.0305,  3.1567],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6956, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1912, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -24\n",
      " Episode 220 \t avg length: 29 \t reward: -26\n",
      " Episode 240 \t avg length: 29 \t reward: -22\n",
      " Episode 260 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.1974, 0.1973, 0.1952,  ..., 0.1752, 0.1740, 0.1729])\n",
      "Advantages :  tensor([-0.9606, -0.9736, -1.0225,  ...,  2.8780,  2.9372,  2.9967],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6956, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1911, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -18\n",
      " Episode 300 \t avg length: 29 \t reward: -22\n",
      " Episode 320 \t avg length: 29 \t reward: -24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values :  tensor([0.1974, 0.1973, 0.1952,  ..., 0.1792, 0.1794, 0.1786])\n",
      "Advantages :  tensor([-0.9867, -1.0004, -1.0521,  ...,  2.9809,  3.0881,  3.1719],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6957, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1911, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -25\n",
      " Episode 360 \t avg length: 29 \t reward: -31\n",
      " Episode 380 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.1974, 0.1973, 0.1952,  ..., 0.1932, 0.1939, 0.1919])\n",
      "Advantages :  tensor([-0.7975, -0.8101, -0.8573,  ...,  3.0551,  2.9398,  2.9525],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6950, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1909, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -24\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -30\n",
      " Episode 460 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.1974, 0.1973, 0.1952,  ..., 0.1902, 0.1896, 0.1910])\n",
      "Advantages :  tensor([-0.7421, -0.7542, -0.7994,  ...,  2.9016,  2.8798,  2.9516],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6959, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1910, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -17\n",
      " Episode 500 \t avg length: 29 \t reward: -17\n",
      " Episode 20 \t avg length: 29 \t reward: -32\n",
      " Episode 40 \t avg length: 29 \t reward: -30\n",
      " Episode 60 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([ 0.0494,  0.0516,  0.0480,  ..., -0.0196, -0.0245, -0.0254])\n",
      "Advantages :  tensor([-0.2175, -0.2372, -0.2967,  ...,  2.8349,  2.9224,  2.8913],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5814, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0864, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -33\n",
      " Episode 100 \t avg length: 29 \t reward: -27\n",
      " Episode 120 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.0494, 0.0516, 0.0480,  ..., 0.1141, 0.1171, 0.1145])\n",
      "Advantages :  tensor([-0.1922, -0.2136, -0.2793,  ...,  2.9233,  2.9892,  3.0512],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5775, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0906, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -33\n",
      " Episode 160 \t avg length: 29 \t reward: -25\n",
      " Episode 180 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.0494, 0.0516, 0.0480,  ..., 0.1241, 0.1236, 0.1244])\n",
      "Advantages :  tensor([-0.3270, -0.3481, -0.4132,  ...,  2.9361,  2.7980,  2.8547],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5768, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0886, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -18\n",
      " Episode 220 \t avg length: 29 \t reward: -23\n",
      " Episode 240 \t avg length: 29 \t reward: -19\n",
      " Episode 260 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.0494, 0.0516, 0.0480,  ..., 0.1025, 0.1063, 0.1082])\n",
      "Advantages :  tensor([-0.4862, -0.5086, -0.5781,  ...,  2.8933,  2.9123,  2.9682],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5773, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0891, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -21\n",
      " Episode 300 \t avg length: 29 \t reward: -18\n",
      " Episode 320 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.0494, 0.0516, 0.0480,  ..., 0.0205, 0.0162, 0.0159])\n",
      "Advantages :  tensor([-0.5705, -0.5939, -0.6668,  ...,  3.1015,  3.0859,  3.1006],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5777, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0910, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -23\n",
      " Episode 360 \t avg length: 29 \t reward: -10\n",
      " Episode 380 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([0.0494, 0.0516, 0.0480,  ..., 0.1589, 0.1561, 0.1540])\n",
      "Advantages :  tensor([-0.6668, -0.6889, -0.7572,  ...,  2.6147,  2.6669,  2.7213],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5790, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0902, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -21\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -25\n",
      " Episode 460 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.0494, 0.0516, 0.0480,  ..., 0.0930, 0.0896, 0.0900])\n",
      "Advantages :  tensor([-0.6987, -0.7212, -0.7912,  ...,  2.7227,  2.7444,  2.7717],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5788, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0900, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -19\n",
      " Episode 500 \t avg length: 29 \t reward: -18\n",
      "Sorting Parent Indexes:  [1 0]\n",
      " Data Type:  <class 'numpy.ndarray'>\n",
      "Sorting Completed\n",
      "Selecting Top Parents\n",
      "Generation  0  | Mean rewards:  -0.04954732239165646  | Mean of top 5:  0.04117593048870491\n",
      "Top  2  scores [1 0]\n",
      "Rewards for top:  [tensor(0.1202, dtype=torch.float64), tensor(-0.0379, dtype=torch.float64)]\n",
      " Episode 20 \t avg length: 29 \t reward: -24\n",
      " Episode 40 \t avg length: 29 \t reward: -23\n",
      " Episode 60 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.1793, 0.1787, 0.1754,  ..., 0.2780, 0.2795, 0.2848])\n",
      "Advantages :  tensor([-1.1869, -1.2769, -1.1885,  ...,  3.8258,  3.9246,  4.1095],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7053, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1978, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -29\n",
      " Episode 100 \t avg length: 29 \t reward: -18\n",
      " Episode 120 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([0.1793, 0.1787, 0.1754,  ..., 0.1496, 0.1498, 0.1535])\n",
      "Advantages :  tensor([-0.7381, -0.7995, -0.7379,  ...,  3.1524,  3.1774,  3.1512],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7208, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2036, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -27\n",
      " Episode 180 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([0.1793, 0.1787, 0.1754,  ..., 0.2074, 0.2072, 0.2067])\n",
      "Advantages :  tensor([-0.5288, -0.5885, -0.5285,  ...,  3.2706,  3.3022,  3.1220],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7212, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2039, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -29\n",
      " Episode 220 \t avg length: 29 \t reward: -22\n",
      " Episode 240 \t avg length: 29 \t reward: -19\n",
      " Episode 260 \t avg length: 29 \t reward: -33\n",
      "State Values :  tensor([0.1793, 0.1787, 0.1754,  ..., 0.1673, 0.1712, 0.1745])\n",
      "Advantages :  tensor([-0.5388, -0.5936, -0.5383,  ...,  2.9549,  2.9141,  2.8811],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7231, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2052, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -26\n",
      " Episode 300 \t avg length: 29 \t reward: -42\n",
      " Episode 320 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.1793, 0.1787, 0.1754,  ..., 0.1859, 0.1841, 0.1836])\n",
      "Advantages :  tensor([-0.3906, -0.4434, -0.3899,  ...,  2.9527,  2.8126,  2.8642],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7270, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2080, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -25\n",
      " Episode 360 \t avg length: 29 \t reward: -27\n",
      " Episode 380 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([0.1793, 0.1787, 0.1754,  ..., 0.2521, 0.2563, 0.2619])\n",
      "Advantages :  tensor([-0.4004, -0.4538, -0.3998,  ...,  2.4925,  2.6067,  2.7414],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7277, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2088, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -31\n",
      " Episode 420 \t avg length: 29 \t reward: -20\n",
      " Episode 440 \t avg length: 29 \t reward: -30\n",
      " Episode 460 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.1793, 0.1787, 0.1754,  ..., 0.1519, 0.1555, 0.1554])\n",
      "Advantages :  tensor([-0.3987, -0.4529, -0.3981,  ...,  2.9545,  2.9523,  2.9956],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7295, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2104, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -30\n",
      " Episode 500 \t avg length: 29 \t reward: -24\n",
      "Score for elite i  1  is  tensor(-0.2050, dtype=torch.float64)\n",
      " Episode 20 \t avg length: 29 \t reward: -19\n",
      " Episode 40 \t avg length: 29 \t reward: -21\n",
      " Episode 60 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.1553, -0.1535, -0.1518,  ..., -0.1530, -0.1512, -0.1494])\n",
      "Advantages :  tensor([0.7469, 0.7165, 0.7072,  ..., 2.7343, 2.7446, 2.8057],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3537, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1450, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 80 \t avg length: 29 \t reward: -27\n",
      " Episode 100 \t avg length: 29 \t reward: -21\n",
      " Episode 120 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.1553, -0.1535, -0.1518,  ..., -0.1796, -0.1776, -0.1762])\n",
      "Advantages :  tensor([1.3954, 1.3645, 1.3551,  ..., 3.5991, 3.5544, 3.6007],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3548, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1438, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -29\n",
      " Episode 160 \t avg length: 29 \t reward: -24\n",
      " Episode 180 \t avg length: 29 \t reward: -35\n",
      "State Values :  tensor([-0.1553, -0.1535, -0.1518,  ..., -0.1423, -0.1412, -0.1396])\n",
      "Advantages :  tensor([1.4421, 1.4134, 1.4045,  ..., 3.4587, 3.4823, 3.4718],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3540, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1441, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -21\n",
      " Episode 220 \t avg length: 29 \t reward: -28\n",
      " Episode 240 \t avg length: 29 \t reward: -23\n",
      " Episode 260 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([-0.1553, -0.1535, -0.1518,  ..., -0.1209, -0.1196, -0.1206])\n",
      "Advantages :  tensor([1.5900, 1.5598, 1.5505,  ..., 3.6487, 3.6919, 3.6474],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3530, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1446, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -29\n",
      " Episode 300 \t avg length: 29 \t reward: -18\n",
      " Episode 320 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.1553, -0.1535, -0.1518,  ..., -0.1425, -0.1435, -0.1419])\n",
      "Advantages :  tensor([1.5731, 1.5434, 1.5343,  ..., 3.6342, 3.5782, 3.5656],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3537, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1446, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -31\n",
      " Episode 360 \t avg length: 29 \t reward: -28\n",
      " Episode 380 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.1553, -0.1535, -0.1518,  ..., -0.1341, -0.1324, -0.1304])\n",
      "Advantages :  tensor([1.6020, 1.5721, 1.5630,  ..., 3.4382, 3.5607, 3.6124],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3545, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1447, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -21\n",
      " Episode 420 \t avg length: 29 \t reward: -25\n",
      " Episode 440 \t avg length: 29 \t reward: -17\n",
      " Episode 460 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([-0.1553, -0.1535, -0.1518,  ..., -0.1161, -0.1148, -0.1137])\n",
      "Advantages :  tensor([1.5496, 1.5200, 1.5110,  ..., 3.2991, 3.3525, 3.4465],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3542, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.1440, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -29\n",
      "Score for elite i  0  is  tensor(0.1444, dtype=torch.float64)\n",
      "Elite selected with index  0  and score tensor(0.1444, dtype=torch.float64)\n",
      " Episode 20 \t avg length: 29 \t reward: -25\n",
      " Episode 40 \t avg length: 29 \t reward: -27\n",
      " Episode 60 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.0356, -0.0353, -0.0341,  ...,  0.0381,  0.0378,  0.0377])\n",
      "Advantages :  tensor([-0.4900, -0.5245, -0.6323,  ...,  3.7229,  3.8276,  3.8604],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4858, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0083, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -24\n",
      " Episode 100 \t avg length: 29 \t reward: -29\n",
      " Episode 120 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0356, -0.0353, -0.0341,  ...,  0.0254,  0.0251,  0.0265])\n",
      "Advantages :  tensor([-0.2834, -0.3168, -0.4211,  ...,  3.6785,  3.7340,  3.7804],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4912, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0108, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -25\n",
      " Episode 160 \t avg length: 29 \t reward: -22\n",
      " Episode 180 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.0356, -0.0353, -0.0341,  ...,  0.0336,  0.0337,  0.0338])\n",
      "Advantages :  tensor([-0.3135, -0.3447, -0.4424,  ...,  3.3824,  3.4584,  3.5600],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4976, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0122, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -17\n",
      " Episode 220 \t avg length: 29 \t reward: -29\n",
      " Episode 240 \t avg length: 29 \t reward: -34\n",
      " Episode 260 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.0356, -0.0353, -0.0341,  ..., -0.0363, -0.0357, -0.0351])\n",
      "Advantages :  tensor([-0.2157, -0.2446, -0.3351,  ...,  3.3307,  3.3698,  3.4079],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4967, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0107, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -17\n",
      " Episode 300 \t avg length: 29 \t reward: -22\n",
      " Episode 320 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.0356, -0.0353, -0.0341,  ...,  0.0775,  0.0770,  0.0766])\n",
      "Advantages :  tensor([-0.3337, -0.3638, -0.4582,  ...,  3.0834,  3.1737,  3.2688],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4942, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0103, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -25\n",
      " Episode 360 \t avg length: 29 \t reward: -18\n",
      " Episode 380 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0356, -0.0353, -0.0341,  ...,  0.0129,  0.0129,  0.0128])\n",
      "Advantages :  tensor([-0.3639, -0.3944, -0.4897,  ...,  3.3667,  3.3002,  3.3152],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4964, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0102, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -22\n",
      " Episode 420 \t avg length: 29 \t reward: -21\n",
      " Episode 440 \t avg length: 29 \t reward: -19\n",
      " Episode 460 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([-0.0356, -0.0353, -0.0341,  ..., -0.0182, -0.0179, -0.0199])\n",
      "Advantages :  tensor([-0.3740, -0.4048, -0.5010,  ...,  3.4744,  3.5304,  3.5123],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4973, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0113, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -27\n",
      " Episode 500 \t avg length: 29 \t reward: -18\n",
      " Episode 20 \t avg length: 29 \t reward: -15\n",
      " Episode 40 \t avg length: 29 \t reward: -13\n",
      " Episode 60 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([-0.1018, -0.1005, -0.1007,  ..., -0.1058, -0.1035, -0.1021])\n",
      "Advantages :  tensor([1.1737, 1.0684, 0.9940,  ..., 2.8163, 2.8477, 2.7203],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4205, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0678, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -19\n",
      " Episode 100 \t avg length: 29 \t reward: -26\n",
      " Episode 120 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.1018, -0.1005, -0.1007,  ..., -0.0815, -0.0786, -0.0784])\n",
      "Advantages :  tensor([1.3334, 1.2471, 1.1863,  ..., 2.5473, 2.6286, 2.4739],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4079, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0768, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -16\n",
      " Episode 160 \t avg length: 29 \t reward: -18\n",
      " Episode 180 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.1018, -0.1005, -0.1007,  ..., -0.0561, -0.0568, -0.0539])\n",
      "Advantages :  tensor([1.6219, 1.5297, 1.4647,  ..., 2.9738, 2.7924, 2.8281],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4082, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0761, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -24\n",
      " Episode 220 \t avg length: 29 \t reward: -24\n",
      " Episode 240 \t avg length: 29 \t reward: -22\n",
      " Episode 260 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([-0.1018, -0.1005, -0.1007,  ..., -0.1767, -0.1764, -0.1763])\n",
      "Advantages :  tensor([1.5842, 1.4963, 1.4342,  ..., 3.0470, 3.0173, 2.9677],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4131, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0731, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -15\n",
      " Episode 300 \t avg length: 29 \t reward: -25\n",
      " Episode 320 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([-0.1018, -0.1005, -0.1007,  ..., -0.0125, -0.0106, -0.0095])\n",
      "Advantages :  tensor([1.6668, 1.5766, 1.5129,  ..., 2.5938, 2.6666, 2.7961],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4096, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0755, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 360 \t avg length: 29 \t reward: -24\n",
      " Episode 380 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.1018, -0.1005, -0.1007,  ..., -0.0977, -0.0958, -0.0931])\n",
      "Advantages :  tensor([1.8316, 1.7363, 1.6691,  ..., 2.8480, 2.9389, 3.0852],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4133, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0745, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -22\n",
      " Episode 420 \t avg length: 29 \t reward: -27\n",
      " Episode 440 \t avg length: 29 \t reward: -25\n",
      " Episode 460 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.1018, -0.1005, -0.1007,  ..., -0.1295, -0.1286, -0.1270])\n",
      "Advantages :  tensor([1.8677, 1.7738, 1.7075,  ..., 3.2696, 3.2494, 3.2225],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4112, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0754, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -14\n",
      " Episode 500 \t avg length: 29 \t reward: -24\n",
      " Episode 20 \t avg length: 29 \t reward: -18\n",
      " Episode 40 \t avg length: 29 \t reward: -26\n",
      " Episode 60 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([ 0.0149,  0.0130,  0.0173,  ..., -0.1039, -0.1071, -0.1087])\n",
      "Advantages :  tensor([0.6704, 0.6973, 0.8576,  ..., 3.1126, 3.1532, 3.1451],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3978, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0913, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -28\n",
      " Episode 100 \t avg length: 29 \t reward: -20\n",
      " Episode 120 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([ 0.0149,  0.0130,  0.0173,  ..., -0.0826, -0.0787, -0.0799])\n",
      "Advantages :  tensor([0.7348, 0.7607, 0.9145,  ..., 3.1512, 3.0178, 3.0843],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4015, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0884, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -27\n",
      " Episode 160 \t avg length: 29 \t reward: -22\n",
      " Episode 180 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([ 0.0149,  0.0130,  0.0173,  ..., -0.1505, -0.1557, -0.1574])\n",
      "Advantages :  tensor([0.5971, 0.6224, 0.7721,  ..., 2.8827, 2.9892, 3.0212],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4025, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0878, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -23\n",
      " Episode 220 \t avg length: 29 \t reward: -22\n",
      " Episode 240 \t avg length: 29 \t reward: -19\n",
      " Episode 260 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([ 0.0149,  0.0130,  0.0173,  ..., -0.1211, -0.1254, -0.1304])\n",
      "Advantages :  tensor([0.6294, 0.6568, 0.8203,  ..., 2.9162, 3.0194, 3.1396],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3999, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0898, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -23\n",
      " Episode 300 \t avg length: 29 \t reward: -17\n",
      " Episode 320 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([ 0.0149,  0.0130,  0.0173,  ..., -0.0493, -0.0521, -0.0535])\n",
      "Advantages :  tensor([0.5528, 0.5783, 0.7297,  ..., 2.9751, 2.9199, 2.8268],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4010, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0892, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -27\n",
      " Episode 360 \t avg length: 29 \t reward: -29\n",
      " Episode 380 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([ 0.0149,  0.0130,  0.0173,  ..., -0.1127, -0.1165, -0.1173])\n",
      "Advantages :  tensor([0.6599, 0.6868, 0.8472,  ..., 3.0528, 3.1359, 3.1421],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4003, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0895, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -29\n",
      " Episode 420 \t avg length: 29 \t reward: -20\n",
      " Episode 440 \t avg length: 29 \t reward: -22\n",
      " Episode 460 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([ 0.0149,  0.0130,  0.0173,  ..., -0.0614, -0.0587, -0.0622])\n",
      "Advantages :  tensor([0.6505, 0.6774, 0.8374,  ..., 3.2610, 3.1678, 3.1807],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4010, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0892, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -25\n",
      " Episode 20 \t avg length: 29 \t reward: -19\n",
      " Episode 40 \t avg length: 29 \t reward: -19\n",
      " Episode 60 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([0.2532, 0.2536, 0.2555,  ..., 0.2762, 0.2787, 0.2781])\n",
      "Advantages :  tensor([-0.2444, -0.3439, -0.4724,  ...,  2.5548,  2.6594,  2.7058],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7742, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2572, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -16\n",
      " Episode 100 \t avg length: 29 \t reward: -15\n",
      " Episode 120 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([0.2532, 0.2536, 0.2555,  ..., 0.2640, 0.2659, 0.2660])\n",
      "Advantages :  tensor([-0.2004, -0.2941, -0.4152,  ...,  2.5816,  2.6250,  2.6151],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7739, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2576, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -24\n",
      " Episode 160 \t avg length: 29 \t reward: -20\n",
      " Episode 180 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.2532, 0.2536, 0.2555,  ..., 0.2453, 0.2434, 0.2413])\n",
      "Advantages :  tensor([-0.2957, -0.3882, -0.5080,  ...,  2.3193,  2.3727,  2.4394],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7748, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2572, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -17\n",
      " Episode 220 \t avg length: 29 \t reward: -30\n",
      " Episode 240 \t avg length: 29 \t reward: -23\n",
      " Episode 260 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.2532, 0.2536, 0.2555,  ..., 0.2617, 0.2625, 0.2605])\n",
      "Advantages :  tensor([-0.2046, -0.2980, -0.4188,  ...,  2.6581,  2.6720,  2.7137],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7751, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2573, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -24\n",
      " Episode 300 \t avg length: 29 \t reward: -23\n",
      " Episode 320 \t avg length: 29 \t reward: -15\n",
      "State Values :  tensor([0.2532, 0.2536, 0.2555,  ..., 0.2563, 0.2574, 0.2571])\n",
      "Advantages :  tensor([-0.2095, -0.3039, -0.4259,  ...,  2.6373,  2.6524,  2.6234],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7756, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2574, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -19\n",
      " Episode 360 \t avg length: 29 \t reward: -20\n",
      " Episode 380 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.2532, 0.2536, 0.2555,  ..., 0.2689, 0.2691, 0.2680])\n",
      "Advantages :  tensor([-0.1732, -0.2669, -0.3881,  ...,  2.5140,  2.5695,  2.6062],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7748, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2572, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -20\n",
      " Episode 420 \t avg length: 29 \t reward: -25\n",
      " Episode 440 \t avg length: 29 \t reward: -26\n",
      " Episode 460 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([0.2532, 0.2536, 0.2555,  ..., 0.2553, 0.2565, 0.2580])\n",
      "Advantages :  tensor([-0.1242, -0.2182, -0.3398,  ...,  2.7716,  2.7991,  2.8209],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7747, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2571, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -23\n",
      " Episode 500 \t avg length: 29 \t reward: -28\n",
      "Sorting Parent Indexes:  [2 1]\n",
      " Data Type:  <class 'numpy.ndarray'>\n",
      "Sorting Completed\n",
      "Selecting Top Parents\n",
      "Generation  1  | Mean rewards:  -0.026099144646629435  | Mean of top 5:  0.08169917521547199\n",
      "Top  2  scores [2 1]\n",
      "Rewards for top:  [tensor(0.0893, dtype=torch.float64), tensor(0.0741, dtype=torch.float64)]\n",
      " Episode 20 \t avg length: 29 \t reward: -23\n",
      " Episode 40 \t avg length: 29 \t reward: -24\n",
      " Episode 60 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.2981, -0.2999, -0.2997,  ..., -0.2615, -0.2663, -0.2688])\n",
      "Advantages :  tensor([1.1697, 1.2073, 1.2795,  ..., 3.6005, 3.5262, 3.4764],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2468, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.2783, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -17\n",
      " Episode 100 \t avg length: 29 \t reward: -18\n",
      " Episode 120 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.2981, -0.2999, -0.2997,  ..., -0.2759, -0.2760, -0.2812])\n",
      "Advantages :  tensor([1.0349, 1.0760, 1.1551,  ..., 3.5397, 3.6055, 3.5663],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2468, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.2788, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 140 \t avg length: 29 \t reward: -18\n",
      " Episode 160 \t avg length: 29 \t reward: -17\n",
      " Episode 180 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.2981, -0.2999, -0.2997,  ..., -0.3071, -0.3086, -0.3067])\n",
      "Advantages :  tensor([1.0140, 1.0571, 1.1405,  ..., 3.5210, 3.6379, 3.5556],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2478, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.2788, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -20\n",
      " Episode 220 \t avg length: 29 \t reward: -23\n",
      " Episode 240 \t avg length: 29 \t reward: -18\n",
      " Episode 260 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.2981, -0.2999, -0.2997,  ..., -0.2527, -0.2579, -0.2580])\n",
      "Advantages :  tensor([0.9855, 1.0270, 1.1069,  ..., 3.5066, 3.4390, 3.4448],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2470, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.2796, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -18\n",
      " Episode 300 \t avg length: 29 \t reward: -17\n",
      " Episode 320 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.2981, -0.2999, -0.2997,  ..., -0.2763, -0.2758, -0.2743])\n",
      "Advantages :  tensor([1.0043, 1.0495, 1.1371,  ..., 3.7501, 3.7699, 3.7667],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2473, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.2808, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -22\n",
      " Episode 360 \t avg length: 29 \t reward: -19\n",
      " Episode 380 \t avg length: 29 \t reward: -33\n",
      "State Values :  tensor([-0.2981, -0.2999, -0.2997,  ..., -0.2681, -0.2720, -0.2706])\n",
      "Advantages :  tensor([1.0149, 1.0571, 1.1384,  ..., 3.2694, 3.4177, 3.4741],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2457, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.2814, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -17\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -21\n",
      " Episode 460 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.2981, -0.2999, -0.2997,  ..., -0.2618, -0.2607, -0.2602])\n",
      "Advantages :  tensor([1.0194, 1.0602, 1.1389,  ..., 3.4897, 3.4635, 3.4390],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2465, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.2805, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -22\n",
      " Episode 500 \t avg length: 29 \t reward: -23\n",
      "Score for elite i  2  is  tensor(0.2797, dtype=torch.float64)\n",
      " Episode 20 \t avg length: 29 \t reward: -26\n",
      " Episode 40 \t avg length: 29 \t reward: -17\n",
      " Episode 60 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.0983, 0.0985, 0.0994,  ..., 0.1067, 0.1059, 0.1061])\n",
      "Advantages :  tensor([-0.4705, -0.5405, -0.5630,  ...,  2.8139,  2.8510,  2.8515],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5908, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1007, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -27\n",
      " Episode 100 \t avg length: 29 \t reward: -23\n",
      " Episode 120 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([0.0983, 0.0985, 0.0994,  ..., 0.0708, 0.0729, 0.0750])\n",
      "Advantages :  tensor([-0.0795, -0.1516, -0.1749,  ...,  3.6751,  3.5862,  3.4958],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5926, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1010, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -25\n",
      " Episode 160 \t avg length: 29 \t reward: -38\n",
      " Episode 180 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.0983, 0.0985, 0.0994,  ..., 0.1151, 0.1169, 0.1157])\n",
      "Advantages :  tensor([0.2471, 0.1905, 0.1720,  ..., 2.8087, 2.9307, 2.9194],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5931, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1015, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -26\n",
      " Episode 220 \t avg length: 29 \t reward: -22\n",
      " Episode 240 \t avg length: 29 \t reward: -24\n",
      " Episode 260 \t avg length: 29 \t reward: -33\n",
      "State Values :  tensor([0.0983, 0.0985, 0.0994,  ..., 0.1020, 0.1030, 0.1025])\n",
      "Advantages :  tensor([0.3228, 0.2664, 0.2480,  ..., 3.0213, 3.0357, 3.0841],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5932, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1013, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -39\n",
      " Episode 300 \t avg length: 29 \t reward: -23\n",
      " Episode 320 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([0.0983, 0.0985, 0.0994,  ..., 0.0766, 0.0783, 0.0781])\n",
      "Advantages :  tensor([0.3631, 0.3096, 0.2922,  ..., 3.0755, 3.0192, 3.0643],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5941, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1017, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -16\n",
      " Episode 360 \t avg length: 29 \t reward: -22\n",
      " Episode 380 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.0983, 0.0985, 0.0994,  ..., 0.1266, 0.1257, 0.1272])\n",
      "Advantages :  tensor([0.2860, 0.2312, 0.2134,  ..., 3.0479, 2.9491, 2.9879],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5933, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1007, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -26\n",
      " Episode 420 \t avg length: 29 \t reward: -21\n",
      " Episode 440 \t avg length: 29 \t reward: -30\n",
      " Episode 460 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([0.0983, 0.0985, 0.0994,  ..., 0.1278, 0.1265, 0.1251])\n",
      "Advantages :  tensor([0.3086, 0.2539, 0.2360,  ..., 3.2253, 3.0869, 2.8927],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5932, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1006, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -24\n",
      "Score for elite i  1  is  tensor(-0.1011, dtype=torch.float64)\n",
      " Episode 20 \t avg length: 29 \t reward: -21\n",
      " Episode 40 \t avg length: 29 \t reward: -21\n",
      " Episode 60 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.1609, 0.1606, 0.1640,  ..., 0.1543, 0.1531, 0.1513])\n",
      "Advantages :  tensor([-0.0422,  0.0997,  0.0234,  ...,  3.4008,  3.3075,  3.2339],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6594, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1621, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -24\n",
      " Episode 100 \t avg length: 29 \t reward: -19\n",
      " Episode 120 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.1609, 0.1606, 0.1640,  ..., 0.1927, 0.1954, 0.1973])\n",
      "Advantages :  tensor([-0.2510, -0.0913, -0.1768,  ...,  3.3256,  3.3303,  3.3343],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6626, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1620, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -26\n",
      " Episode 180 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.1609, 0.1606, 0.1640,  ..., 0.1942, 0.1972, 0.2001])\n",
      "Advantages :  tensor([0.0429, 0.1990, 0.1154,  ..., 3.1680, 3.2562, 3.3482],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6593, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1612, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -30\n",
      " Episode 220 \t avg length: 29 \t reward: -29\n",
      " Episode 240 \t avg length: 29 \t reward: -28\n",
      " Episode 260 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([0.1609, 0.1606, 0.1640,  ..., 0.1844, 0.1868, 0.1896])\n",
      "Advantages :  tensor([0.1100, 0.2554, 0.1773,  ..., 3.3196, 3.3150, 3.2881],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6612, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1620, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -24\n",
      " Episode 300 \t avg length: 29 \t reward: -23\n",
      " Episode 320 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.1609, 0.1606, 0.1640,  ..., 0.1682, 0.1681, 0.1713])\n",
      "Advantages :  tensor([0.1176, 0.2665, 0.1866,  ..., 3.3158, 3.3969, 3.3789],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6626, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1629, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -27\n",
      " Episode 360 \t avg length: 29 \t reward: -22\n",
      " Episode 380 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.1609, 0.1606, 0.1640,  ..., 0.2287, 0.2300, 0.2298])\n",
      "Advantages :  tensor([0.0475, 0.1989, 0.1177,  ..., 3.3612, 3.3905, 3.3636],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6630, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1636, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 420 \t avg length: 29 \t reward: -25\n",
      " Episode 440 \t avg length: 29 \t reward: -21\n",
      " Episode 460 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.1609, 0.1606, 0.1640,  ..., 0.1766, 0.1800, 0.1782])\n",
      "Advantages :  tensor([0.0450, 0.1969, 0.1154,  ..., 3.4920, 3.4674, 3.4293],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6632, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.1635, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -21\n",
      "Score for elite i  3  is  tensor(-0.1624, dtype=torch.float64)\n",
      "Elite selected with index  2  and score tensor(0.2797, dtype=torch.float64)\n",
      " Episode 20 \t avg length: 29 \t reward: -19\n",
      " Episode 40 \t avg length: 29 \t reward: -27\n",
      " Episode 60 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.2658, 0.2651, 0.2666,  ..., 0.2733, 0.2750, 0.2734])\n",
      "Advantages :  tensor([-0.4221, -0.3903, -0.3342,  ...,  2.5576,  2.5721,  2.5487],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7839, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2628, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -27\n",
      " Episode 100 \t avg length: 29 \t reward: -23\n",
      " Episode 120 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([0.2658, 0.2651, 0.2666,  ..., 0.2739, 0.2742, 0.2744])\n",
      "Advantages :  tensor([-0.1972, -0.1560, -0.0826,  ...,  3.5762,  3.5981,  3.7209],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7871, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2641, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -25\n",
      " Episode 180 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.2658, 0.2651, 0.2666,  ..., 0.2715, 0.2707, 0.2680])\n",
      "Advantages :  tensor([-0.1788, -0.1348, -0.0561,  ...,  3.7601,  3.9156,  4.0579],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7875, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2648, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -27\n",
      " Episode 220 \t avg length: 29 \t reward: -24\n",
      " Episode 240 \t avg length: 29 \t reward: -18\n",
      " Episode 260 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.2658, 0.2651, 0.2666,  ..., 0.2839, 0.2824, 0.2840])\n",
      "Advantages :  tensor([-0.2620, -0.2223, -0.1518,  ...,  3.5650,  3.4866,  3.4889],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7864, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2645, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -21\n",
      " Episode 300 \t avg length: 29 \t reward: -21\n",
      " Episode 320 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.2658, 0.2651, 0.2666,  ..., 0.2657, 0.2674, 0.2688])\n",
      "Advantages :  tensor([-0.3036, -0.2607, -0.1841,  ...,  3.5105,  3.6412,  3.7796],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7867, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2641, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -21\n",
      " Episode 360 \t avg length: 29 \t reward: -28\n",
      " Episode 380 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.2658, 0.2651, 0.2666,  ..., 0.2471, 0.2475, 0.2461])\n",
      "Advantages :  tensor([-0.3430, -0.3021, -0.2295,  ...,  3.4795,  3.4606,  3.4865],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7865, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2641, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -20\n",
      " Episode 420 \t avg length: 29 \t reward: -30\n",
      " Episode 440 \t avg length: 29 \t reward: -25\n",
      " Episode 460 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.2658, 0.2651, 0.2666,  ..., 0.2815, 0.2820, 0.2843])\n",
      "Advantages :  tensor([-0.3171, -0.2784, -0.2097,  ...,  3.0724,  3.1259,  3.2423],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7866, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.2643, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -24\n",
      " Episode 500 \t avg length: 29 \t reward: -31\n",
      " Episode 20 \t avg length: 29 \t reward: -23\n",
      " Episode 40 \t avg length: 29 \t reward: -24\n",
      " Episode 60 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.0708, -0.0723, -0.0743,  ..., -0.0808, -0.0830, -0.0827])\n",
      "Advantages :  tensor([-0.9643, -0.9393, -0.8860,  ...,  3.0549,  3.0641,  3.1329],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4150, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0731, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -17\n",
      " Episode 100 \t avg length: 29 \t reward: -24\n",
      " Episode 120 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([-0.0708, -0.0723, -0.0743,  ..., -0.0758, -0.0755, -0.0755])\n",
      "Advantages :  tensor([-1.1464, -1.1200, -1.0634,  ...,  3.1797,  3.2008,  3.2077],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4154, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0733, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -21\n",
      " Episode 160 \t avg length: 29 \t reward: -28\n",
      " Episode 180 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([-0.0708, -0.0723, -0.0743,  ..., -0.0815, -0.0822, -0.0832])\n",
      "Advantages :  tensor([-0.8727, -0.8466, -0.7909,  ...,  3.3216,  3.3328,  3.4147],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4156, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0733, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -22\n",
      " Episode 220 \t avg length: 29 \t reward: -22\n",
      " Episode 240 \t avg length: 29 \t reward: -18\n",
      " Episode 260 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([-0.0708, -0.0723, -0.0743,  ..., -0.0732, -0.0758, -0.0780])\n",
      "Advantages :  tensor([-0.7807, -0.7558, -0.7028,  ...,  3.1741,  3.2202,  3.2694],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4153, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0733, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -30\n",
      " Episode 300 \t avg length: 29 \t reward: -26\n",
      " Episode 320 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([-0.0708, -0.0723, -0.0743,  ..., -0.0683, -0.0684, -0.0700])\n",
      "Advantages :  tensor([-0.6124, -0.5893, -0.5401,  ...,  3.2150,  3.1383,  3.0545],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4155, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0733, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -18\n",
      " Episode 360 \t avg length: 29 \t reward: -27\n",
      " Episode 380 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.0708, -0.0723, -0.0743,  ..., -0.0793, -0.0794, -0.0792])\n",
      "Advantages :  tensor([-0.6264, -0.6032, -0.5536,  ...,  3.1041,  3.0950,  3.1915],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4153, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0734, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -24\n",
      " Episode 420 \t avg length: 29 \t reward: -30\n",
      " Episode 440 \t avg length: 29 \t reward: -21\n",
      " Episode 460 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.0708, -0.0723, -0.0743,  ..., -0.0724, -0.0728, -0.0732])\n",
      "Advantages :  tensor([-0.6004, -0.5765, -0.5256,  ...,  3.3588,  3.3900,  3.4218],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4154, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0734, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -23\n",
      " Episode 500 \t avg length: 29 \t reward: -21\n",
      " Episode 20 \t avg length: 29 \t reward: -18\n",
      " Episode 40 \t avg length: 29 \t reward: -25\n",
      " Episode 60 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([-0.0520, -0.0508, -0.0517,  ..., -0.1044, -0.1040, -0.1080])\n",
      "Advantages :  tensor([2.5944, 2.5056, 2.4189,  ..., 3.3205, 3.4098, 3.5370],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4028, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0855, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -25\n",
      " Episode 100 \t avg length: 29 \t reward: -23\n",
      " Episode 120 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([-0.0520, -0.0508, -0.0517,  ..., -0.0779, -0.0756, -0.0792])\n",
      "Advantages :  tensor([2.1866, 2.1053, 2.0260,  ..., 2.8394, 2.9503, 3.0430],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4056, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0849, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -20\n",
      " Episode 160 \t avg length: 29 \t reward: -20\n",
      " Episode 180 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.0520, -0.0508, -0.0517,  ..., -0.0734, -0.0706, -0.0701])\n",
      "Advantages :  tensor([2.3683, 2.2828, 2.1993,  ..., 3.2422, 3.3615, 3.3192],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4046, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0850, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 220 \t avg length: 29 \t reward: -22\n",
      " Episode 240 \t avg length: 29 \t reward: -27\n",
      " Episode 260 \t avg length: 29 \t reward: -33\n",
      "State Values :  tensor([-0.0520, -0.0508, -0.0517,  ..., -0.0854, -0.0867, -0.0881])\n",
      "Advantages :  tensor([2.4910, 2.4059, 2.3228,  ..., 3.4497, 3.5343, 3.6172],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4059, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0844, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -19\n",
      " Episode 300 \t avg length: 29 \t reward: -25\n",
      " Episode 320 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0520, -0.0508, -0.0517,  ..., -0.0534, -0.0523, -0.0543])\n",
      "Advantages :  tensor([2.6640, 2.5743, 2.4867,  ..., 3.4480, 3.6385, 3.6707],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4077, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0833, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -34\n",
      " Episode 360 \t avg length: 29 \t reward: -23\n",
      " Episode 380 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0520, -0.0508, -0.0517,  ..., -0.1007, -0.0989, -0.0982])\n",
      "Advantages :  tensor([2.6783, 2.5887, 2.5010,  ..., 3.7409, 3.5814, 3.6530],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4078, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0827, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -25\n",
      " Episode 420 \t avg length: 29 \t reward: -22\n",
      " Episode 440 \t avg length: 29 \t reward: -23\n",
      " Episode 460 \t avg length: 29 \t reward: -15\n",
      "State Values :  tensor([-0.0520, -0.0508, -0.0517,  ..., -0.0282, -0.0276, -0.0271])\n",
      "Advantages :  tensor([2.5770, 2.4885, 2.4019,  ..., 3.3295, 3.3896, 3.4527],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4078, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0829, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -24\n",
      " Episode 500 \t avg length: 29 \t reward: -14\n",
      " Episode 20 \t avg length: 29 \t reward: -29\n",
      " Episode 40 \t avg length: 29 \t reward: -24\n",
      " Episode 60 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([-0.0524, -0.0541, -0.0519,  ..., -0.0658, -0.0633, -0.0672])\n",
      "Advantages :  tensor([-0.6419, -0.6487, -0.7152,  ...,  3.5471,  3.5263,  3.4968],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4472, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0440, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -31\n",
      " Episode 100 \t avg length: 29 \t reward: -21\n",
      " Episode 120 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.0524, -0.0541, -0.0519,  ..., -0.0489, -0.0502, -0.0516])\n",
      "Advantages :  tensor([-0.8192, -0.8267, -0.8978,  ...,  3.3483,  3.4233,  3.4966],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4476, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0447, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -22\n",
      " Episode 160 \t avg length: 29 \t reward: -23\n",
      " Episode 180 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0524, -0.0541, -0.0519,  ..., -0.0522, -0.0531, -0.0541])\n",
      "Advantages :  tensor([-0.9344, -0.9428, -1.0214,  ...,  3.6087,  3.7167,  3.8243],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4484, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0438, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -28\n",
      " Episode 220 \t avg length: 29 \t reward: -28\n",
      " Episode 240 \t avg length: 29 \t reward: -26\n",
      " Episode 260 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0524, -0.0541, -0.0519,  ..., -0.0660, -0.0634, -0.0693])\n",
      "Advantages :  tensor([-0.8950, -0.9036, -0.9834,  ...,  4.1624,  3.9333,  3.9727],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4486, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0428, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -24\n",
      " Episode 300 \t avg length: 29 \t reward: -28\n",
      " Episode 320 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0524, -0.0541, -0.0519,  ..., -0.0819, -0.0875, -0.0899])\n",
      "Advantages :  tensor([-0.9105, -0.9192, -0.9997,  ...,  4.0017,  4.0155,  4.0644],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4495, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0422, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -25\n",
      " Episode 360 \t avg length: 29 \t reward: -27\n",
      " Episode 380 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.0524, -0.0541, -0.0519,  ..., -0.0364, -0.0348, -0.0328])\n",
      "Advantages :  tensor([-0.9088, -0.9179, -1.0017,  ...,  4.2781,  4.0716,  4.1615],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4486, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0421, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -27\n",
      " Episode 420 \t avg length: 29 \t reward: -22\n",
      " Episode 440 \t avg length: 29 \t reward: -27\n",
      " Episode 460 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.0524, -0.0541, -0.0519,  ..., -0.0844, -0.0902, -0.0899])\n",
      "Advantages :  tensor([-0.9144, -0.9233, -1.0059,  ...,  3.9789,  4.0391,  4.1779],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4497, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0414, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -27\n",
      " Episode 500 \t avg length: 29 \t reward: -22\n",
      "Sorting Parent Indexes:  [2 1]\n",
      " Data Type:  <class 'numpy.ndarray'>\n",
      "Sorting Completed\n",
      "Selecting Top Parents\n",
      "Generation  2  | Mean rewards:  -0.015863546799270975  | Mean of top 5:  0.07875056564592749\n",
      "Top  2  scores [2 1]\n",
      "Rewards for top:  [tensor(0.0842, dtype=torch.float64), tensor(0.0733, dtype=torch.float64)]\n",
      " Episode 20 \t avg length: 29 \t reward: -18\n",
      " Episode 40 \t avg length: 29 \t reward: -15\n",
      " Episode 60 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.0320, 0.0353, 0.0348,  ..., 0.0860, 0.0874, 0.0865])\n",
      "Advantages :  tensor([-0.7451, -0.9261, -0.9761,  ...,  3.4006,  3.4465,  3.5705],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5203, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0348, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -22\n",
      " Episode 100 \t avg length: 29 \t reward: -23\n",
      " Episode 120 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.0320, 0.0353, 0.0348,  ..., 0.0431, 0.0443, 0.0435])\n",
      "Advantages :  tensor([-0.0846, -0.2133, -0.2484,  ...,  3.0664,  3.0361,  3.0525],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5177, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0319, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -16\n",
      " Episode 160 \t avg length: 29 \t reward: -19\n",
      " Episode 180 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([0.0320, 0.0353, 0.0348,  ..., 0.0721, 0.0752, 0.0783])\n",
      "Advantages :  tensor([ 0.1513,  0.0263, -0.0077,  ...,  3.2188,  3.1876,  3.1486],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5221, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0333, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -31\n",
      " Episode 220 \t avg length: 29 \t reward: -25\n",
      " Episode 240 \t avg length: 29 \t reward: -19\n",
      " Episode 260 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([ 0.0320,  0.0353,  0.0348,  ..., -0.0255, -0.0225, -0.0250])\n",
      "Advantages :  tensor([0.2187, 0.0874, 0.0515,  ..., 3.1207, 3.2164, 3.3835],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5220, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0331, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -19\n",
      " Episode 300 \t avg length: 29 \t reward: -27\n",
      " Episode 320 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([0.0320, 0.0353, 0.0348,  ..., 0.0615, 0.0646, 0.0684])\n",
      "Advantages :  tensor([0.3279, 0.1948, 0.1585,  ..., 3.2800, 3.3397, 3.4046],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5231, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0345, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -30\n",
      " Episode 360 \t avg length: 29 \t reward: -27\n",
      " Episode 380 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.0320, 0.0353, 0.0348,  ..., 0.0386, 0.0387, 0.0414])\n",
      "Advantages :  tensor([0.4010, 0.2738, 0.2390,  ..., 3.4732, 3.3140, 3.3756],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5218, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0340, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -26\n",
      " Episode 420 \t avg length: 29 \t reward: -21\n",
      " Episode 440 \t avg length: 29 \t reward: -20\n",
      " Episode 460 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([ 0.0320,  0.0353,  0.0348,  ..., -0.0050, -0.0071, -0.0094])\n",
      "Advantages :  tensor([0.3482, 0.2214, 0.1868,  ..., 3.4474, 3.5205, 3.4482],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5216, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0336, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -27\n",
      "Score for elite i  2  is  tensor(-0.0336, dtype=torch.float64)\n",
      " Episode 20 \t avg length: 29 \t reward: -30\n",
      " Episode 40 \t avg length: 29 \t reward: -29\n",
      " Episode 60 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.0696, 0.0622, 0.0605,  ..., 0.0096, 0.0122, 0.0146])\n",
      "Advantages :  tensor([-1.0240, -1.0718, -1.0750,  ...,  3.6210,  3.6364,  3.6471],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5349, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0466, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -23\n",
      " Episode 100 \t avg length: 29 \t reward: -24\n",
      " Episode 120 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([0.0696, 0.0622, 0.0605,  ..., 0.0600, 0.0549, 0.0572])\n",
      "Advantages :  tensor([-1.1525, -1.2038, -1.2073,  ...,  3.8936,  3.9353,  3.8856],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5347, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0452, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -30\n",
      " Episode 160 \t avg length: 29 \t reward: -20\n",
      " Episode 180 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([0.0696, 0.0622, 0.0605,  ..., 0.0304, 0.0274, 0.0269])\n",
      "Advantages :  tensor([-1.2593, -1.3123, -1.3160,  ...,  3.9051,  3.9268,  3.8333],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5361, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0461, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -23\n",
      " Episode 220 \t avg length: 29 \t reward: -19\n",
      " Episode 240 \t avg length: 29 \t reward: -26\n",
      " Episode 260 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.0696, 0.0622, 0.0605,  ..., 0.0661, 0.0598, 0.0602])\n",
      "Advantages :  tensor([-1.3426, -1.3952, -1.3989,  ...,  3.7372,  3.7047,  3.7327],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5374, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0468, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -26\n",
      " Episode 300 \t avg length: 29 \t reward: -17\n",
      " Episode 320 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.0696, 0.0622, 0.0605,  ..., 0.0335, 0.0317, 0.0299])\n",
      "Advantages :  tensor([-1.3728, -1.4228, -1.4262,  ...,  3.4031,  3.4628,  3.5210],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5371, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0461, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -29\n",
      " Episode 360 \t avg length: 29 \t reward: -27\n",
      " Episode 380 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.0696, 0.0622, 0.0605,  ..., 0.0731, 0.0742, 0.0677])\n",
      "Advantages :  tensor([-1.3381, -1.3880, -1.3914,  ...,  3.3691,  3.5068,  3.5159],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5375, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0474, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -21\n",
      " Episode 420 \t avg length: 29 \t reward: -21\n",
      " Episode 440 \t avg length: 29 \t reward: -29\n",
      " Episode 460 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([0.0696, 0.0622, 0.0605,  ..., 0.0794, 0.0796, 0.0827])\n",
      "Advantages :  tensor([-1.2897, -1.3400, -1.3434,  ...,  3.4250,  3.5490,  3.5818],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5371, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(-0.0474, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -23\n",
      "Score for elite i  1  is  tensor(-0.0465, dtype=torch.float64)\n",
      " Episode 20 \t avg length: 29 \t reward: -22\n",
      " Episode 40 \t avg length: 29 \t reward: -26\n",
      " Episode 60 \t avg length: 29 \t reward: -12\n",
      "State Values :  tensor([-0.0383, -0.0451, -0.0519,  ..., -0.1840, -0.1899, -0.1958])\n",
      "Advantages :  tensor([1.0104, 0.9889, 0.9769,  ..., 2.2187, 2.3074, 2.4048],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4798, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0360, dtype=torch.float64)\n",
      " Episode 80 \t avg length: 29 \t reward: -23\n",
      " Episode 100 \t avg length: 29 \t reward: -25\n",
      " Episode 120 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0383, -0.0451, -0.0519,  ..., -0.0545, -0.0628, -0.0683])\n",
      "Advantages :  tensor([1.4024, 1.3753, 1.3595,  ..., 2.7495, 2.8642, 2.9533],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4674, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0469, dtype=torch.float64)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -13\n",
      " Episode 180 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.0383, -0.0451, -0.0519,  ..., -0.2321, -0.2258, -0.2249])\n",
      "Advantages :  tensor([1.3739, 1.3469, 1.3313,  ..., 3.3339, 3.1810, 3.0701],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4635, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0443, dtype=torch.float64)\n",
      " Episode 200 \t avg length: 29 \t reward: -21\n",
      " Episode 220 \t avg length: 29 \t reward: -29\n",
      " Episode 240 \t avg length: 29 \t reward: -30\n",
      " Episode 260 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.0383, -0.0451, -0.0519,  ...,  0.0385,  0.0450,  0.0412])\n",
      "Advantages :  tensor([1.5260, 1.4989, 1.4831,  ..., 2.9913, 3.0695, 3.0336],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4693, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0385, dtype=torch.float64)\n",
      " Episode 280 \t avg length: 29 \t reward: -27\n",
      " Episode 300 \t avg length: 29 \t reward: -20\n",
      " Episode 320 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0383, -0.0451, -0.0519,  ...,  0.0679,  0.0610,  0.0643])\n",
      "Advantages :  tensor([1.5618, 1.5337, 1.5173,  ..., 2.9221, 2.9708, 3.0888],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4639, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0406, dtype=torch.float64)\n",
      " Episode 340 \t avg length: 29 \t reward: -20\n",
      " Episode 360 \t avg length: 29 \t reward: -26\n",
      " Episode 380 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0383, -0.0451, -0.0519,  ..., -0.1681, -0.1641, -0.1610])\n",
      "Advantages :  tensor([1.5831, 1.5543, 1.5374,  ..., 3.3236, 3.2675, 3.2704],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4639, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0396, dtype=torch.float64)\n",
      " Episode 400 \t avg length: 29 \t reward: -18\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -24\n",
      " Episode 460 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.0383, -0.0451, -0.0519,  ..., -0.1102, -0.1181, -0.1261])\n",
      "Advantages :  tensor([1.5646, 1.5363, 1.5197,  ..., 3.1264, 3.2002, 3.2745],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4652, dtype=torch.float64)\n",
      "returned Surrogate Loss  tensor(0.0387, dtype=torch.float64)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -29\n",
      "Score for elite i  3  is  tensor(0.0408, dtype=torch.float64)\n",
      "Elite selected with index  3  and score tensor(0.0408, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "game_actions = 2 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "#torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 4\n",
    "agents = return_random_agents(num_agents)\n",
    "\n",
    "# How many top agents to consider as parents\n",
    "top_limit = 2\n",
    "\n",
    "# run evolution until X generations\n",
    "generations = 3\n",
    "\n",
    "elite_index = None\n",
    "\n",
    "for generation in range(generations):\n",
    "\n",
    "    # return rewards of agents\n",
    "    rewards = run_agents_n_times(agents, 1) #return average of 3 runs\n",
    "\n",
    "    # sort by rewards\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]#reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "    print(\"Sorting Parent Indexes: \",sorted_parent_indexes)\n",
    "    print(\" Data Type: \", type(sorted_parent_indexes))\n",
    "    print(\"Sorting Completed\")\n",
    "    print(\"Selecting Top Parents\")\n",
    "    \n",
    "    top_rewards = []\n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "    print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "    #print(rewards)\n",
    "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    \n",
    "    # setup an empty list for containing children agents\n",
    "    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "\n",
    "    # kill all agents, and replace them with their children\n",
    "    agents = children_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        \n",
    "        env_record = Monitor(env, './video', force=True)\n",
    "        observation = env_record.reset()\n",
    "        last_observation = observation\n",
    "        r=0\n",
    "        for _ in range(250):\n",
    "            env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env_record.step(action)\n",
    "            r=r+reward\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        env_record.close()\n",
    "        print(\"Rewards: \",r)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' and 'p' must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-5e4c93bb9c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-335-b8ca255d9fc6>\u001b[0m in \u001b[0;36mplay_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' and 'p' must have same size"
     ]
    }
   ],
   "source": [
    "play_agent(agents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
