{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "    ##Changed this part    \n",
    "    def forward(self, inputs):\n",
    "            x = self.action_layer(inputs)\n",
    "            return x\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device) \n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        surr_loss = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        #print(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        #print(\"Rewardsd Mean: \") \n",
    "        #print(rewards.mean())\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            print(\"State Values : \", state_values.detach())\n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            print(\"Advantages : \", advantages)\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            surr_loss.append(loss)\n",
    "            print(\"L Clip : \", loss.mean())\n",
    "            \n",
    "            # take gradient step\n",
    "            #self.optimizer.zero_grad()\n",
    "            #loss.mean().backward()\n",
    "            #self.optimizer.step()\n",
    "            \n",
    "                  \n",
    "        # Copy new weights into old policy:       \n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        #print(\"Surrogate Loss Mean: \",surr1.mean())\n",
    "        return surr1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_reward = 30         # stop training if avg_reward > solved_reward\n",
    "log_interval = 20           # print avg reward in the interval\n",
    "max_episodes = 500        # max training episodes\n",
    "max_timesteps = 30         # max timesteps in one episode\n",
    "n_latent_var = 64           # number of variables in hidden layer\n",
    "update_timestep = 2000      # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 1                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "random_seed = None\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_dim = 4\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_agents(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        ##Change this part\n",
    "        agent = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "         ##Commented this part   \n",
    "        #init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "        \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried to change this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    total_loss = []\n",
    "    env_name = \"LunarLander-v2\"\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    if random_seed:\n",
    "        torch.manual_seed(random_seed)\n",
    "        env.seed(random_seed)\n",
    "    \n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "    #print(lr,betas)\n",
    "    \n",
    "    # logging variables\n",
    "    running_reward = 0\n",
    "    avg_length = 0\n",
    "    timestep = 0\n",
    "    surri=0\n",
    "    s1 = []\n",
    "    s2 = 0\n",
    "    s_temp = []\n",
    "    surr_actual = []\n",
    "    \n",
    "    # training loop\n",
    "    #for agent in agents:\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        #agent.eval()\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            timestep += 1\n",
    "            \n",
    "            # Running policy_old:\n",
    "            action = ppo.policy_old.act(state, memory)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Saving reward and is_terminal:\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            # update if its time\n",
    "            if timestep % update_timestep == 0:\n",
    "                surri=ppo.update(memory)\n",
    "                print(\"returned Surrogate Loss \", surri)\n",
    "                #memory.clear_memory()\n",
    "                timestep = 0\n",
    "            \n",
    "            running_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        avg_length += t\n",
    "        \n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n",
    "            break\n",
    "            \n",
    "        # logging\n",
    "        if i_episode % log_interval == 0:\n",
    "            avg_length = int(avg_length/log_interval)\n",
    "            running_reward = int((running_reward/log_interval))\n",
    "            print(' Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
    "            running_reward = 0\n",
    "            avg_length = 0\n",
    "        s1.append(surri)\n",
    "        #surr_mean = torch.mean(torch.stack(surr_actual))    \n",
    "    s1 = [i for i in s1 if i != 0]\n",
    "    #print(torch.mean(torch.stack(s1)))\n",
    "    #print(\"\\n\")\n",
    "            \n",
    "    #print(total_loss)\n",
    "    return torch.mean(torch.stack(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    #score = 0.\n",
    "    #for i in range(runs):\n",
    "    #print(run_agents([agent]))\n",
    "    score = run_agents([agent])\n",
    "    return score#/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <--Till Now Only worked -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    avg_score = []\n",
    "    for agent in agents:\n",
    "        avg_score.append(return_average_score(agent,runs))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    \n",
    "    mutation_power = 0.02 #hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "    \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            \n",
    "                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                                \n",
    "                                    \n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    \n",
    "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
    "    for i in range(len(agents)-1):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    #now add one elite\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index=len(children_agents)-1 #it is the last one\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    \n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if(elite_index is not None):\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 20 \t avg length: 29 \t reward: -23\n",
      " Episode 40 \t avg length: 29 \t reward: -17\n",
      " Episode 60 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.0507, 0.0542, 0.0543,  ..., 0.0867, 0.0867, 0.0842])\n",
      "Advantages :  tensor([-0.1833, -0.3635, -0.3604,  ...,  2.7754,  2.7542,  2.7764],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5846, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0928, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -26\n",
      " Episode 100 \t avg length: 29 \t reward: -24\n",
      " Episode 120 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.0507, 0.0542, 0.0543,  ..., 0.0651, 0.0660, 0.0631])\n",
      "Advantages :  tensor([0.2647, 0.0899, 0.0928,  ..., 3.0370, 3.0281, 3.1138],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5860, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0914, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -28\n",
      " Episode 160 \t avg length: 29 \t reward: -25\n",
      " Episode 180 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.0507, 0.0542, 0.0543,  ..., 0.1110, 0.1122, 0.1072])\n",
      "Advantages :  tensor([0.2861, 0.1257, 0.1284,  ..., 2.8719, 2.9502, 2.9719],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5827, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0911, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -28\n",
      " Episode 220 \t avg length: 29 \t reward: -24\n",
      " Episode 240 \t avg length: 29 \t reward: -19\n",
      " Episode 260 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.0507, 0.0542, 0.0543,  ..., 0.0537, 0.0479, 0.0470])\n",
      "Advantages :  tensor([0.3225, 0.1575, 0.1602,  ..., 2.8118, 2.9063, 2.9874],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5856, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0934, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -25\n",
      " Episode 300 \t avg length: 29 \t reward: -24\n",
      " Episode 320 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.0507, 0.0542, 0.0543,  ..., 0.1791, 0.1808, 0.1824])\n",
      "Advantages :  tensor([0.2995, 0.1324, 0.1352,  ..., 2.7635, 2.8398, 2.9231],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5848, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0933, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -19\n",
      " Episode 360 \t avg length: 29 \t reward: -18\n",
      " Episode 380 \t avg length: 29 \t reward: -32\n",
      "State Values :  tensor([0.0507, 0.0542, 0.0543,  ..., 0.0927, 0.0965, 0.0908])\n",
      "Advantages :  tensor([0.3240, 0.1548, 0.1577,  ..., 3.2943, 3.0858, 3.1752],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5863, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0934, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -20\n",
      " Episode 420 \t avg length: 29 \t reward: -29\n",
      " Episode 440 \t avg length: 29 \t reward: -28\n",
      " Episode 460 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.0507, 0.0542, 0.0543,  ..., 0.1053, 0.1069, 0.1080])\n",
      "Advantages :  tensor([0.3757, 0.2045, 0.2074,  ..., 2.9104, 2.9943, 3.0830],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5860, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0931, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -22\n",
      " Episode 500 \t avg length: 29 \t reward: -22\n",
      " Episode 20 \t avg length: 29 \t reward: -21\n",
      " Episode 40 \t avg length: 29 \t reward: -21\n",
      " Episode 60 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.2786, -0.2782, -0.2774,  ..., -0.2313, -0.2321, -0.2306])\n",
      "Advantages :  tensor([-0.5062, -0.6139, -0.6190,  ...,  3.1155,  3.1127,  3.0610],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2877, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -25\n",
      " Episode 100 \t avg length: 29 \t reward: -25\n",
      " Episode 120 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.2786, -0.2782, -0.2774,  ..., -0.2857, -0.2848, -0.2839])\n",
      "Advantages :  tensor([-0.4178, -0.5377, -0.5432,  ...,  3.3593,  3.4445,  3.5311],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2840, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -26\n",
      " Episode 160 \t avg length: 29 \t reward: -19\n",
      " Episode 180 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([-0.2786, -0.2782, -0.2774,  ..., -0.1851, -0.1821, -0.1805])\n",
      "Advantages :  tensor([-0.3610, -0.4757, -0.4810,  ...,  3.4966,  3.4300,  3.4460],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2417, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2830, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -29\n",
      " Episode 220 \t avg length: 29 \t reward: -17\n",
      " Episode 240 \t avg length: 29 \t reward: -26\n",
      " Episode 260 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.2786, -0.2782, -0.2774,  ..., -0.2356, -0.2309, -0.2286])\n",
      "Advantages :  tensor([-0.3347, -0.4493, -0.4546,  ...,  3.2556,  3.3507,  3.3998],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2387, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -30\n",
      " Episode 300 \t avg length: 29 \t reward: -28\n",
      " Episode 320 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([-0.2786, -0.2782, -0.2774,  ..., -0.3196, -0.3194, -0.3197])\n",
      "Advantages :  tensor([-0.2914, -0.4019, -0.4070,  ...,  3.5428,  3.4839,  3.5752],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2400, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2874, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -19\n",
      " Episode 360 \t avg length: 29 \t reward: -21\n",
      " Episode 380 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.2786, -0.2782, -0.2774,  ..., -0.3388, -0.3398, -0.3413])\n",
      "Advantages :  tensor([-0.2648, -0.3762, -0.3814,  ...,  3.3995,  3.4753,  3.5632],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2400, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -28\n",
      " Episode 420 \t avg length: 29 \t reward: -28\n",
      " Episode 440 \t avg length: 29 \t reward: -27\n",
      " Episode 460 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.2786, -0.2782, -0.2774,  ..., -0.2317, -0.2260, -0.2250])\n",
      "Advantages :  tensor([-0.2296, -0.3426, -0.3479,  ...,  3.4284,  3.5166,  3.5675],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2873, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -25\n",
      " Episode 20 \t avg length: 29 \t reward: -21\n",
      " Episode 40 \t avg length: 29 \t reward: -33\n",
      " Episode 60 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.1413, 0.1441, 0.1438,  ..., 0.1392, 0.1439, 0.1396])\n",
      "Advantages :  tensor([0.4263, 0.5016, 0.4711,  ..., 2.7616, 2.8890, 2.9990],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -23\n",
      " Episode 100 \t avg length: 29 \t reward: -28\n",
      " Episode 120 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.1413, 0.1441, 0.1438,  ..., 0.0401, 0.0402, 0.0381])\n",
      "Advantages :  tensor([0.4097, 0.4777, 0.4501,  ..., 2.8949, 2.8359, 2.8558],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -24\n",
      " Episode 160 \t avg length: 29 \t reward: -19\n",
      " Episode 180 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([0.1413, 0.1441, 0.1438,  ..., 0.0832, 0.0749, 0.0688])\n",
      "Advantages :  tensor([0.2809, 0.3570, 0.3263,  ..., 2.6487, 2.7635, 2.8733],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 220 \t avg length: 29 \t reward: -24\n",
      " Episode 240 \t avg length: 29 \t reward: -23\n",
      " Episode 260 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([0.1413, 0.1441, 0.1438,  ..., 0.0794, 0.0819, 0.0757])\n",
      "Advantages :  tensor([0.3890, 0.4710, 0.4380,  ..., 3.1137, 3.1878, 3.2302],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6014, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -29\n",
      " Episode 300 \t avg length: 29 \t reward: -28\n",
      " Episode 320 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.1413, 0.1441, 0.1438,  ..., 0.0809, 0.0840, 0.0784])\n",
      "Advantages :  tensor([0.4771, 0.5580, 0.5254,  ..., 3.1549, 3.2310, 3.2533],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1077, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -28\n",
      " Episode 360 \t avg length: 29 \t reward: -18\n",
      " Episode 380 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([0.1413, 0.1441, 0.1438,  ..., 0.1238, 0.1168, 0.1195])\n",
      "Advantages :  tensor([0.4502, 0.5320, 0.4990,  ..., 3.0524, 3.1607, 3.1783],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1071, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -25\n",
      " Episode 420 \t avg length: 29 \t reward: -24\n",
      " Episode 440 \t avg length: 29 \t reward: -21\n",
      " Episode 460 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.1413, 0.1441, 0.1438,  ..., 0.1214, 0.1203, 0.1137])\n",
      "Advantages :  tensor([0.4308, 0.5148, 0.4809,  ..., 3.0509, 3.1140, 3.2375],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6016, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -19\n",
      " Episode 500 \t avg length: 29 \t reward: -22\n",
      " Episode 20 \t avg length: 29 \t reward: -16\n",
      " Episode 40 \t avg length: 29 \t reward: -22\n",
      " Episode 60 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0848, -0.0846, -0.0841,  ..., -0.0922, -0.0915, -0.0913])\n",
      "Advantages :  tensor([1.9160, 1.9153, 1.9603,  ..., 3.6409, 3.6294, 3.7128],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0891, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -19\n",
      " Episode 100 \t avg length: 29 \t reward: -29\n",
      " Episode 120 \t avg length: 29 \t reward: -15\n",
      "State Values :  tensor([-0.0848, -0.0846, -0.0841,  ..., -0.1062, -0.1055, -0.1047])\n",
      "Advantages :  tensor([1.8176, 1.8169, 1.8584,  ..., 3.4476, 3.4799, 3.5589],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0886, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -25\n",
      " Episode 160 \t avg length: 29 \t reward: -16\n",
      " Episode 180 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0848, -0.0846, -0.0841,  ..., -0.1136, -0.1133, -0.1130])\n",
      "Advantages :  tensor([1.8124, 1.8117, 1.8534,  ..., 3.6484, 3.6663, 3.6600],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -23\n",
      " Episode 220 \t avg length: 29 \t reward: -23\n",
      " Episode 240 \t avg length: 29 \t reward: -22\n",
      " Episode 260 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0848, -0.0846, -0.0841,  ..., -0.1031, -0.1020, -0.1012])\n",
      "Advantages :  tensor([1.9977, 1.9970, 2.0404,  ..., 3.5691, 3.5735, 3.7097],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0902, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -26\n",
      " Episode 300 \t avg length: 29 \t reward: -20\n",
      " Episode 320 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([-0.0848, -0.0846, -0.0841,  ..., -0.0845, -0.0858, -0.0862])\n",
      "Advantages :  tensor([1.9139, 1.9132, 1.9561,  ..., 3.5764, 3.7109, 3.7709],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0901, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -18\n",
      " Episode 360 \t avg length: 29 \t reward: -26\n",
      " Episode 380 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([-0.0848, -0.0846, -0.0841,  ..., -0.0824, -0.0812, -0.0803])\n",
      "Advantages :  tensor([1.7576, 1.7569, 1.7973,  ..., 3.4674, 3.5223, 3.4463],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -19\n",
      " Episode 420 \t avg length: 29 \t reward: -20\n",
      " Episode 440 \t avg length: 29 \t reward: -19\n",
      " Episode 460 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([-0.0848, -0.0846, -0.0841,  ..., -0.0867, -0.0856, -0.0848])\n",
      "Advantages :  tensor([1.7604, 1.7597, 1.8011,  ..., 3.5957, 3.5542, 3.4507],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0901, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -18\n",
      " Episode 500 \t avg length: 29 \t reward: -18\n",
      "REWARDS:::::: [tensor(-0.0926, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.2861, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(-0.1073, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.0898, dtype=torch.float64, grad_fn=<MeanBackward0>)]\n",
      "Sorting Parent Indexes:  [1 3]\n",
      " Data Type:  <class 'numpy.ndarray'>\n",
      "Sorting Completed\n",
      "Selecting Top Parents\n",
      "Generation  0  | Mean rewards:  tensor(0.0440, dtype=torch.float64, grad_fn=<MeanBackward0>)  | Mean of top 5:  tensor(0.1880, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Top  2  scores [1 3]\n",
      "Rewards for top:  [tensor(0.2861, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.0898, dtype=torch.float64, grad_fn=<MeanBackward0>)]\n",
      " Episode 20 \t avg length: 29 \t reward: -17\n",
      " Episode 40 \t avg length: 29 \t reward: -26\n",
      " Episode 60 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.2446, -0.2462, -0.2469,  ..., -0.2103, -0.2097, -0.2097])\n",
      "Advantages :  tensor([-0.2530, -0.1829, -0.1142,  ...,  2.7872,  2.7862,  2.8370],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -21\n",
      " Episode 100 \t avg length: 29 \t reward: -20\n",
      " Episode 120 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.2446, -0.2462, -0.2469,  ..., -0.2244, -0.2256, -0.2238])\n",
      "Advantages :  tensor([-0.1119, -0.0426,  0.0253,  ...,  2.8439,  2.8504,  2.8942],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2505, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -24\n",
      " Episode 160 \t avg length: 29 \t reward: -21\n",
      " Episode 180 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([-0.2446, -0.2462, -0.2469,  ..., -0.2568, -0.2533, -0.2550])\n",
      "Advantages :  tensor([0.0176, 0.0882, 0.1575,  ..., 3.1460, 3.2297, 3.0661],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2693, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -27\n",
      " Episode 220 \t avg length: 29 \t reward: -25\n",
      " Episode 240 \t avg length: 29 \t reward: -21\n",
      " Episode 260 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([-0.2446, -0.2462, -0.2469,  ..., -0.2702, -0.2715, -0.2707])\n",
      "Advantages :  tensor([0.0590, 0.1306, 0.2009,  ..., 3.3446, 3.2941, 3.3172],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2496, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 280 \t avg length: 29 \t reward: -21\n",
      " Episode 300 \t avg length: 29 \t reward: -31\n",
      " Episode 320 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([-0.2446, -0.2462, -0.2469,  ..., -0.2886, -0.2910, -0.2939])\n",
      "Advantages :  tensor([0.1626, 0.2374, 0.3108,  ..., 3.5827, 3.5736, 3.5775],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2678, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2492, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -20\n",
      " Episode 360 \t avg length: 29 \t reward: -20\n",
      " Episode 380 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.2446, -0.2462, -0.2469,  ..., -0.2503, -0.2506, -0.2463])\n",
      "Advantages :  tensor([0.1261, 0.2029, 0.2784,  ..., 3.5230, 3.5022, 3.6207],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2492, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -22\n",
      " Episode 420 \t avg length: 29 \t reward: -30\n",
      " Episode 440 \t avg length: 29 \t reward: -28\n",
      " Episode 460 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([-0.2446, -0.2462, -0.2469,  ..., -0.2108, -0.2058, -0.2072])\n",
      "Advantages :  tensor([0.1648, 0.2405, 0.3149,  ..., 3.2562, 3.3929, 3.4430],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2682, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2496, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -24\n",
      " Episode 500 \t avg length: 29 \t reward: -22\n",
      "Score for elite i  1  is  tensor(0.2494, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 20 \t avg length: 29 \t reward: -14\n",
      " Episode 40 \t avg length: 29 \t reward: -30\n",
      " Episode 60 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.1484, 0.1433, 0.1386,  ..., 0.1196, 0.1198, 0.1201])\n",
      "Advantages :  tensor([-0.0117, -0.0322, -0.0490,  ...,  2.1432,  2.1348,  2.2542],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0920, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -18\n",
      " Episode 100 \t avg length: 29 \t reward: -15\n",
      " Episode 120 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.1484, 0.1433, 0.1386,  ..., 0.1263, 0.1218, 0.1255])\n",
      "Advantages :  tensor([-0.1003, -0.1249, -0.1450,  ...,  2.4976,  2.5643,  2.5688],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5885, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0937, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -26\n",
      " Episode 160 \t avg length: 29 \t reward: -16\n",
      " Episode 180 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.1484, 0.1433, 0.1386,  ..., 0.0845, 0.0844, 0.0864])\n",
      "Advantages :  tensor([-0.1268, -0.1520, -0.1726,  ...,  2.9540,  2.8650,  2.8673],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5850, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0943, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -19\n",
      " Episode 220 \t avg length: 29 \t reward: -20\n",
      " Episode 240 \t avg length: 29 \t reward: -17\n",
      " Episode 260 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.1484, 0.1433, 0.1386,  ..., 0.1151, 0.1145, 0.1120])\n",
      "Advantages :  tensor([-0.1069, -0.1306, -0.1500,  ...,  2.4565,  2.5231,  2.5625],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0879, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -23\n",
      " Episode 300 \t avg length: 29 \t reward: -23\n",
      " Episode 320 \t avg length: 29 \t reward: -15\n",
      "State Values :  tensor([0.1484, 0.1433, 0.1386,  ..., 0.1202, 0.1177, 0.1151])\n",
      "Advantages :  tensor([-0.0656, -0.0901, -0.1101,  ...,  2.4038,  2.4790,  2.5567],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5798, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0863, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -28\n",
      " Episode 360 \t avg length: 29 \t reward: -19\n",
      " Episode 380 \t avg length: 29 \t reward: -12\n",
      "State Values :  tensor([0.1484, 0.1433, 0.1386,  ..., 0.1092, 0.1066, 0.1040])\n",
      "Advantages :  tensor([-0.0891, -0.1119, -0.1306,  ...,  2.3959,  2.4529,  2.5106],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0848, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -17\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -16\n",
      " Episode 460 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([0.1484, 0.1433, 0.1386,  ..., 0.0953, 0.0949, 0.0902])\n",
      "Advantages :  tensor([-0.0774, -0.1005, -0.1193,  ...,  2.7277,  2.6361,  2.7386],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5791, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0858, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -12\n",
      " Episode 500 \t avg length: 29 \t reward: -21\n",
      "Score for elite i  3  is  tensor(-0.0895, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Elite selected with index  1  and score tensor(0.2494, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 20 \t avg length: 29 \t reward: -19\n",
      " Episode 40 \t avg length: 29 \t reward: -24\n",
      " Episode 60 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.0506, 0.0509, 0.0492,  ..., 0.0390, 0.0385, 0.0397])\n",
      "Advantages :  tensor([-0.2337, -0.4112, -0.5797,  ...,  3.8088,  3.8829,  4.0779],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5448, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -22\n",
      " Episode 100 \t avg length: 29 \t reward: -21\n",
      " Episode 120 \t avg length: 29 \t reward: -30\n",
      "State Values :  tensor([0.0506, 0.0509, 0.0492,  ..., 0.0191, 0.0206, 0.0184])\n",
      "Advantages :  tensor([ 0.0964, -0.0352, -0.1597,  ...,  3.2985,  3.3936,  3.3896],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0531, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -23\n",
      " Episode 180 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([0.0506, 0.0509, 0.0492,  ..., 0.0654, 0.0643, 0.0666])\n",
      "Advantages :  tensor([ 0.1233, -0.0101, -0.1363,  ...,  3.1645,  3.2647,  3.3305],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5385, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0532, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -14\n",
      " Episode 220 \t avg length: 29 \t reward: -16\n",
      " Episode 240 \t avg length: 29 \t reward: -31\n",
      " Episode 260 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([0.0506, 0.0509, 0.0492,  ..., 0.0124, 0.0103, 0.0100])\n",
      "Advantages :  tensor([ 0.2027,  0.0750, -0.0457,  ...,  3.3319,  3.3084,  3.3171],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5375, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0529, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -30\n",
      " Episode 300 \t avg length: 29 \t reward: -21\n",
      " Episode 320 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([0.0506, 0.0509, 0.0492,  ..., 0.0988, 0.1009, 0.1015])\n",
      "Advantages :  tensor([0.2551, 0.1299, 0.0115,  ..., 3.3194, 3.3493, 3.2820],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5394, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0537, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -19\n",
      " Episode 360 \t avg length: 29 \t reward: -22\n",
      " Episode 380 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.0506, 0.0509, 0.0492,  ..., 0.0550, 0.0534, 0.0538])\n",
      "Advantages :  tensor([ 0.2081,  0.0811, -0.0389,  ...,  3.2899,  3.3921,  3.2515],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 420 \t avg length: 29 \t reward: -21\n",
      " Episode 440 \t avg length: 29 \t reward: -19\n",
      " Episode 460 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.0506, 0.0509, 0.0492,  ..., 0.0184, 0.0175, 0.0188])\n",
      "Advantages :  tensor([ 0.1856,  0.0528, -0.0729,  ...,  3.2083,  3.3792,  3.4184],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5408, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0544, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -27\n",
      " Episode 500 \t avg length: 29 \t reward: -26\n",
      " Episode 20 \t avg length: 29 \t reward: -26\n",
      " Episode 40 \t avg length: 29 \t reward: -23\n",
      " Episode 60 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0843, -0.0776, -0.0813,  ..., -0.0265, -0.0210, -0.0230])\n",
      "Advantages :  tensor([-0.2809, -0.2180, -0.4024,  ...,  2.7669,  2.8840,  2.9451],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3899, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -23\n",
      " Episode 100 \t avg length: 29 \t reward: -19\n",
      " Episode 120 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([-0.0843, -0.0776, -0.0813,  ..., -0.0767, -0.0703, -0.0636])\n",
      "Advantages :  tensor([-0.3092, -0.2394, -0.4424,  ...,  3.1949,  3.2392,  3.3033],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0971, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -26\n",
      " Episode 180 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([-0.0843, -0.0776, -0.0813,  ..., -0.0750, -0.0673, -0.0679])\n",
      "Advantages :  tensor([-0.2503, -0.1862, -0.3738,  ...,  2.9898,  3.0341,  3.0023],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -25\n",
      " Episode 220 \t avg length: 29 \t reward: -18\n",
      " Episode 240 \t avg length: 29 \t reward: -21\n",
      " Episode 260 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.0843, -0.0776, -0.0813,  ..., -0.1852, -0.1867, -0.1802])\n",
      "Advantages :  tensor([-0.2808, -0.2114, -0.4134,  ...,  3.0249,  3.1789,  3.1681],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0950, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -25\n",
      " Episode 300 \t avg length: 29 \t reward: -18\n",
      " Episode 320 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0843, -0.0776, -0.0813,  ..., -0.0926, -0.0862, -0.0836])\n",
      "Advantages :  tensor([-0.2569, -0.1929, -0.3804,  ...,  3.0512,  3.0813,  3.0855],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0961, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -27\n",
      " Episode 360 \t avg length: 29 \t reward: -17\n",
      " Episode 380 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0843, -0.0776, -0.0813,  ..., -0.0592, -0.0537, -0.0486])\n",
      "Advantages :  tensor([-0.2702, -0.2066, -0.3929,  ...,  2.9675,  3.0475,  3.1337],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3947, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -23\n",
      " Episode 420 \t avg length: 29 \t reward: -21\n",
      " Episode 440 \t avg length: 29 \t reward: -24\n",
      " Episode 460 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0843, -0.0776, -0.0813,  ..., -0.1204, -0.1187, -0.1201])\n",
      "Advantages :  tensor([-0.2680, -0.2045, -0.3904,  ...,  3.1492,  3.1578,  3.1452],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0946, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -20\n",
      " Episode 500 \t avg length: 29 \t reward: -26\n",
      " Episode 20 \t avg length: 29 \t reward: -21\n",
      " Episode 40 \t avg length: 29 \t reward: -18\n",
      " Episode 60 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([ 0.0741,  0.0745,  0.0748,  ..., -0.0376, -0.0330, -0.0364])\n",
      "Advantages :  tensor([2.1091, 2.0549, 1.9963,  ..., 2.7990, 2.8102, 2.7779],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0833, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -38\n",
      " Episode 100 \t avg length: 29 \t reward: -24\n",
      " Episode 120 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.0741, 0.0745, 0.0748,  ..., 0.1922, 0.1938, 0.1971])\n",
      "Advantages :  tensor([2.2104, 2.1602, 2.1059,  ..., 2.5708, 2.6524, 2.6905],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -25\n",
      " Episode 160 \t avg length: 29 \t reward: -34\n",
      " Episode 180 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.0741, 0.0745, 0.0748,  ..., 0.0060, 0.0083, 0.0098])\n",
      "Advantages :  tensor([2.1648, 2.1151, 2.0613,  ..., 2.8418, 2.8372, 2.8299],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5933, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0871, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -19\n",
      " Episode 220 \t avg length: 29 \t reward: -15\n",
      " Episode 240 \t avg length: 29 \t reward: -27\n",
      " Episode 260 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([ 0.0741,  0.0745,  0.0748,  ..., -0.0140, -0.0147, -0.0205])\n",
      "Advantages :  tensor([2.2044, 2.1522, 2.0955,  ..., 2.7837, 2.8116, 2.8972],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5943, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0872, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -17\n",
      " Episode 300 \t avg length: 29 \t reward: -24\n",
      " Episode 320 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.0741, 0.0745, 0.0748,  ..., 0.1827, 0.1804, 0.1836])\n",
      "Advantages :  tensor([2.2352, 2.1805, 2.1212,  ..., 2.7657, 2.7447, 2.7817],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5919, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -22\n",
      " Episode 360 \t avg length: 29 \t reward: -21\n",
      " Episode 380 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.0741, 0.0745, 0.0748,  ..., 0.1178, 0.1126, 0.1080])\n",
      "Advantages :  tensor([2.3160, 2.2586, 2.1964,  ..., 2.8713, 2.8968, 2.9193],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5893, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0865, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -23\n",
      " Episode 420 \t avg length: 29 \t reward: -38\n",
      " Episode 440 \t avg length: 29 \t reward: -25\n",
      " Episode 460 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([0.0741, 0.0745, 0.0748,  ..., 0.0115, 0.0109, 0.0107])\n",
      "Advantages :  tensor([2.2613, 2.2073, 2.1487,  ..., 2.8774, 2.8948, 2.9563],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0882, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -16\n",
      " Episode 500 \t avg length: 29 \t reward: -17\n",
      " Episode 20 \t avg length: 29 \t reward: -20\n",
      " Episode 40 \t avg length: 29 \t reward: -31\n",
      " Episode 60 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.2040, -0.2087, -0.2094,  ..., -0.1800, -0.1745, -0.1742])\n",
      "Advantages :  tensor([ 0.0186, -0.0423, -0.0795,  ...,  2.6303,  2.7290,  2.7976],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2983, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -22\n",
      " Episode 100 \t avg length: 29 \t reward: -22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 120 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.2040, -0.2087, -0.2094,  ..., -0.2411, -0.2380, -0.2404])\n",
      "Advantages :  tensor([-0.1817, -0.2503, -0.2919,  ...,  2.9619,  2.9429,  3.0095],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2128, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -19\n",
      " Episode 160 \t avg length: 29 \t reward: -25\n",
      " Episode 180 \t avg length: 29 \t reward: -13\n",
      "State Values :  tensor([-0.2040, -0.2087, -0.2094,  ..., -0.2307, -0.2309, -0.2308])\n",
      "Advantages :  tensor([-0.1862, -0.2599, -0.3043,  ...,  3.2743,  3.2714,  3.2807],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2128, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -27\n",
      " Episode 220 \t avg length: 29 \t reward: -20\n",
      " Episode 240 \t avg length: 29 \t reward: -21\n",
      " Episode 260 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.2040, -0.2087, -0.2094,  ..., -0.2096, -0.2103, -0.2088])\n",
      "Advantages :  tensor([-0.1433, -0.2171, -0.2616,  ...,  3.2309,  3.3124,  3.2719],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3009, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2102, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -19\n",
      " Episode 300 \t avg length: 29 \t reward: -24\n",
      " Episode 320 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.2040, -0.2087, -0.2094,  ..., -0.1967, -0.1957, -0.1904])\n",
      "Advantages :  tensor([-0.1311, -0.2088, -0.2555,  ...,  3.3657,  3.4210,  3.5249],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -23\n",
      " Episode 360 \t avg length: 29 \t reward: -24\n",
      " Episode 380 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.2040, -0.2087, -0.2094,  ..., -0.1370, -0.1349, -0.1287])\n",
      "Advantages :  tensor([-0.1027, -0.1826, -0.2307,  ...,  3.3474,  3.4183,  3.5383],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2099, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -25\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -23\n",
      " Episode 460 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.2040, -0.2087, -0.2094,  ..., -0.2458, -0.2417, -0.2412])\n",
      "Advantages :  tensor([-0.0807, -0.1627, -0.2119,  ...,  3.6464,  3.6196,  3.6549],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.2994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.2103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -26\n",
      " Episode 500 \t avg length: 29 \t reward: -27\n",
      "REWARDS:::::: [tensor(-0.0535, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.0966, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(-0.0863, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.2107, dtype=torch.float64, grad_fn=<MeanBackward0>)]\n",
      "Sorting Parent Indexes:  [3 1]\n",
      " Data Type:  <class 'numpy.ndarray'>\n",
      "Sorting Completed\n",
      "Selecting Top Parents\n",
      "Generation  1  | Mean rewards:  tensor(0.0419, dtype=torch.float64, grad_fn=<MeanBackward0>)  | Mean of top 5:  tensor(0.1537, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Top  2  scores [3 1]\n",
      "Rewards for top:  [tensor(0.2107, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.0966, dtype=torch.float64, grad_fn=<MeanBackward0>)]\n",
      " Episode 20 \t avg length: 29 \t reward: -19\n",
      " Episode 40 \t avg length: 29 \t reward: -16\n",
      " Episode 60 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([-0.0565, -0.0540, -0.0547,  ...,  0.0308,  0.0292,  0.0306])\n",
      "Advantages :  tensor([-1.3585, -1.4391, -1.2667,  ...,  3.3744,  3.2185,  3.2784],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4952, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0138, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -30\n",
      " Episode 100 \t avg length: 29 \t reward: -25\n",
      " Episode 120 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.0565, -0.0540, -0.0547,  ...,  0.0397,  0.0399,  0.0416])\n",
      "Advantages :  tensor([-0.7244, -0.7893, -0.6514,  ...,  2.7788,  2.8459,  2.9249],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4967, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -19\n",
      " Episode 160 \t avg length: 29 \t reward: -26\n",
      " Episode 180 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([-0.0565, -0.0540, -0.0547,  ..., -0.0340, -0.0333, -0.0327])\n",
      "Advantages :  tensor([-0.5480, -0.6099, -0.4786,  ...,  2.9024,  2.9366,  2.9927],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4985, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -25\n",
      " Episode 220 \t avg length: 29 \t reward: -28\n",
      " Episode 240 \t avg length: 29 \t reward: -24\n",
      " Episode 260 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0565, -0.0540, -0.0547,  ...,  0.0394,  0.0394,  0.0408])\n",
      "Advantages :  tensor([-0.4562, -0.5173, -0.3878,  ...,  2.9055,  2.9371,  2.9994],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0164, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -24\n",
      " Episode 300 \t avg length: 29 \t reward: -16\n",
      " Episode 320 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0565, -0.0540, -0.0547,  ..., -0.0004,  0.0018,  0.0026])\n",
      "Advantages :  tensor([-0.5317, -0.5933, -0.4627,  ...,  2.9707,  2.9169,  2.9760],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0167, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -18\n",
      " Episode 360 \t avg length: 29 \t reward: -24\n",
      " Episode 380 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([-0.0565, -0.0540, -0.0547,  ..., -0.0123, -0.0136, -0.0118])\n",
      "Advantages :  tensor([-0.5673, -0.6302, -0.4967,  ...,  3.0092,  3.0360,  3.0120],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5006, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -21\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -21\n",
      " Episode 460 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0565, -0.0540, -0.0547,  ...,  0.0050,  0.0022,  0.0006])\n",
      "Advantages :  tensor([-0.5558, -0.6207, -0.4826,  ...,  3.1878,  3.1497,  3.1453],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0161, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -20\n",
      " Episode 500 \t avg length: 29 \t reward: -20\n",
      "Score for elite i  3  is  tensor(-0.0155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 20 \t avg length: 29 \t reward: -25\n",
      " Episode 40 \t avg length: 29 \t reward: -20\n",
      " Episode 60 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.1316, 0.1322, 0.1327,  ..., 0.1460, 0.1467, 0.1467])\n",
      "Advantages :  tensor([-0.6037, -0.6122, -0.6152,  ...,  2.5271,  2.5283,  2.5009],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1350, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -20\n",
      " Episode 100 \t avg length: 29 \t reward: -16\n",
      " Episode 120 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([0.1316, 0.1322, 0.1327,  ..., 0.1408, 0.1424, 0.1430])\n",
      "Advantages :  tensor([-0.6995, -0.7088, -0.7121,  ...,  2.4898,  2.4176,  2.5557],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 160 \t avg length: 29 \t reward: -20\n",
      " Episode 180 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.1316, 0.1322, 0.1327,  ..., 0.1216, 0.1215, 0.1209])\n",
      "Advantages :  tensor([-0.6465, -0.6555, -0.6587,  ...,  2.6780,  2.7313,  2.6577],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6297, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -20\n",
      " Episode 220 \t avg length: 29 \t reward: -17\n",
      " Episode 240 \t avg length: 29 \t reward: -25\n",
      " Episode 260 \t avg length: 29 \t reward: -30\n",
      "State Values :  tensor([0.1316, 0.1322, 0.1327,  ..., 0.1367, 0.1362, 0.1358])\n",
      "Advantages :  tensor([-0.5536, -0.5626, -0.5658,  ...,  2.5404,  2.6015,  2.7110],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6295, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -26\n",
      " Episode 300 \t avg length: 29 \t reward: -18\n",
      " Episode 320 \t avg length: 29 \t reward: -32\n",
      "State Values :  tensor([0.1316, 0.1322, 0.1327,  ..., 0.1397, 0.1396, 0.1392])\n",
      "Advantages :  tensor([-0.4351, -0.4436, -0.4467,  ...,  2.6076,  2.7196,  2.8055],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1347, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -21\n",
      " Episode 360 \t avg length: 29 \t reward: -17\n",
      " Episode 380 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([0.1316, 0.1322, 0.1327,  ..., 0.1042, 0.1028, 0.1008])\n",
      "Advantages :  tensor([-0.5025, -0.5111, -0.5142,  ...,  2.8881,  2.9043,  2.7745],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -20\n",
      " Episode 420 \t avg length: 29 \t reward: -20\n",
      " Episode 440 \t avg length: 29 \t reward: -24\n",
      " Episode 460 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([0.1316, 0.1322, 0.1327,  ..., 0.1186, 0.1168, 0.1149])\n",
      "Advantages :  tensor([-0.5216, -0.5304, -0.5336,  ...,  2.8876,  2.8698,  2.8534],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.6299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -26\n",
      " Episode 500 \t avg length: 29 \t reward: -22\n",
      "Score for elite i  1  is  tensor(-0.1348, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 20 \t avg length: 29 \t reward: -19\n",
      " Episode 40 \t avg length: 29 \t reward: -32\n",
      " Episode 60 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.0308, 0.0301, 0.0303,  ..., 0.0242, 0.0232, 0.0222])\n",
      "Advantages :  tensor([0.6824, 0.5960, 0.6990,  ..., 2.6386, 2.6891, 2.7538],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5139, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -27\n",
      " Episode 100 \t avg length: 29 \t reward: -23\n",
      " Episode 120 \t avg length: 29 \t reward: -31\n",
      "State Values :  tensor([0.0308, 0.0301, 0.0303,  ..., 0.0251, 0.0250, 0.0256])\n",
      "Advantages :  tensor([0.7751, 0.6761, 0.7941,  ..., 2.9391, 3.0215, 3.0778],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0279, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -22\n",
      " Episode 160 \t avg length: 29 \t reward: -26\n",
      " Episode 180 \t avg length: 29 \t reward: -31\n",
      "State Values :  tensor([0.0308, 0.0301, 0.0303,  ..., 0.0374, 0.0385, 0.0390])\n",
      "Advantages :  tensor([0.7757, 0.6757, 0.7950,  ..., 2.8126, 2.9366, 3.0243],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0279, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -19\n",
      " Episode 220 \t avg length: 29 \t reward: -23\n",
      " Episode 240 \t avg length: 29 \t reward: -27\n",
      " Episode 260 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.0308, 0.0301, 0.0303,  ..., 0.0316, 0.0324, 0.0336])\n",
      "Advantages :  tensor([0.7617, 0.6632, 0.7807,  ..., 3.0274, 3.0284, 3.0521],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -32\n",
      " Episode 300 \t avg length: 29 \t reward: -29\n",
      " Episode 320 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([0.0308, 0.0301, 0.0303,  ..., 0.0295, 0.0294, 0.0288])\n",
      "Advantages :  tensor([0.7802, 0.6817, 0.7992,  ..., 3.1180, 3.1080, 3.1681],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -24\n",
      " Episode 360 \t avg length: 29 \t reward: -26\n",
      " Episode 380 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.0308, 0.0301, 0.0303,  ..., 0.0375, 0.0384, 0.0395])\n",
      "Advantages :  tensor([0.7892, 0.6860, 0.8090,  ..., 3.1058, 3.1593, 3.2019],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -22\n",
      " Episode 420 \t avg length: 29 \t reward: -14\n",
      " Episode 440 \t avg length: 29 \t reward: -24\n",
      " Episode 460 \t avg length: 29 \t reward: -23\n",
      "State Values :  tensor([0.0308, 0.0301, 0.0303,  ..., 0.0292, 0.0292, 0.0294])\n",
      "Advantages :  tensor([0.6999, 0.5975, 0.7196,  ..., 3.0109, 3.0789, 3.1164],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0279, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -20\n",
      " Episode 500 \t avg length: 29 \t reward: -19\n",
      "Score for elite i  3  is  tensor(-0.0278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Elite selected with index  3  and score tensor(-0.0155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 20 \t avg length: 29 \t reward: -22\n",
      " Episode 40 \t avg length: 29 \t reward: -23\n",
      " Episode 60 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.0287, -0.0346, -0.0318,  ..., -0.1351, -0.1312, -0.1305])\n",
      "Advantages :  tensor([-0.4290, -0.5217, -0.4622,  ...,  3.1563,  3.0644,  3.0347],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3985, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0827, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -23\n",
      " Episode 100 \t avg length: 29 \t reward: -25\n",
      " Episode 120 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0287, -0.0346, -0.0318,  ..., -0.0262, -0.0293, -0.0364])\n",
      "Advantages :  tensor([-0.3300, -0.4348, -0.3676,  ...,  3.6646,  3.6086,  3.5130],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0799, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -29\n",
      " Episode 160 \t avg length: 29 \t reward: -25\n",
      " Episode 180 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.0287, -0.0346, -0.0318,  ..., -0.1126, -0.1146, -0.1167])\n",
      "Advantages :  tensor([-0.0709, -0.1691, -0.1061,  ...,  3.3268,  3.4152,  3.5065],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4108, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -28\n",
      " Episode 220 \t avg length: 29 \t reward: -23\n",
      " Episode 240 \t avg length: 29 \t reward: -23\n",
      " Episode 260 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.0287, -0.0346, -0.0318,  ..., -0.0668, -0.0655, -0.0639])\n",
      "Advantages :  tensor([-0.0582, -0.1528, -0.0921,  ...,  3.5422,  3.4964,  3.4420],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4104, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0795, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 280 \t avg length: 29 \t reward: -23\n",
      " Episode 300 \t avg length: 29 \t reward: -25\n",
      " Episode 320 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0287, -0.0346, -0.0318,  ..., -0.1141, -0.1201, -0.1168])\n",
      "Advantages :  tensor([-0.1045, -0.2055, -0.1408,  ...,  3.6680,  3.7615,  3.5958],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0802, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -24\n",
      " Episode 360 \t avg length: 29 \t reward: -22\n",
      " Episode 380 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.0287, -0.0346, -0.0318,  ..., -0.1030, -0.0980, -0.0959])\n",
      "Advantages :  tensor([-0.1341, -0.2386, -0.1716,  ...,  4.0505,  3.8676,  3.9298],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4102, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0790, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -27\n",
      " Episode 420 \t avg length: 29 \t reward: -21\n",
      " Episode 440 \t avg length: 29 \t reward: -21\n",
      " Episode 460 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.0287, -0.0346, -0.0318,  ..., -0.1014, -0.1077, -0.1141])\n",
      "Advantages :  tensor([-0.2145, -0.3167, -0.2512,  ...,  3.4758,  3.5249,  3.5637],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4108, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0781, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -18\n",
      " Episode 500 \t avg length: 29 \t reward: -24\n",
      " Episode 20 \t avg length: 29 \t reward: -25\n",
      " Episode 40 \t avg length: 29 \t reward: -16\n",
      " Episode 60 \t avg length: 29 \t reward: -29\n",
      "State Values :  tensor([-0.0206, -0.0121, -0.0060,  ..., -0.1417, -0.1487, -0.1531])\n",
      "Advantages :  tensor([-0.2266, -0.1765, -0.1196,  ...,  2.9851,  3.0943,  3.1597],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4805, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0222, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -26\n",
      " Episode 100 \t avg length: 29 \t reward: -32\n",
      " Episode 120 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([-0.0206, -0.0121, -0.0060,  ..., -0.0470, -0.0485, -0.0414])\n",
      "Advantages :  tensor([-0.0289,  0.0232,  0.0823,  ...,  3.4202,  3.5616,  3.4998],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0246, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -25\n",
      " Episode 160 \t avg length: 29 \t reward: -25\n",
      " Episode 180 \t avg length: 29 \t reward: -28\n",
      "State Values :  tensor([-0.0206, -0.0121, -0.0060,  ...,  0.0575,  0.0591,  0.0534])\n",
      "Advantages :  tensor([0.0506, 0.1095, 0.1759,  ..., 3.7737, 3.8171, 3.8181],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0233, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -28\n",
      " Episode 220 \t avg length: 29 \t reward: -22\n",
      " Episode 240 \t avg length: 29 \t reward: -23\n",
      " Episode 260 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.0206, -0.0121, -0.0060,  ..., -0.1045, -0.0965, -0.0967])\n",
      "Advantages :  tensor([-0.0283,  0.0311,  0.0980,  ...,  3.7556,  3.7736,  3.8569],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0210, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -25\n",
      " Episode 300 \t avg length: 29 \t reward: -19\n",
      " Episode 320 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([-0.0206, -0.0121, -0.0060,  ..., -0.0318, -0.0378, -0.0375])\n",
      "Advantages :  tensor([-0.0538,  0.0060,  0.0733,  ...,  3.5907,  3.6871,  3.7595],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4695, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0233, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -25\n",
      " Episode 360 \t avg length: 29 \t reward: -25\n",
      " Episode 380 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.0206, -0.0121, -0.0060,  ...,  0.0169,  0.0181,  0.0276])\n",
      "Advantages :  tensor([-0.0879, -0.0303,  0.0346,  ...,  3.2095,  3.3183,  3.4573],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4677, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -19\n",
      " Episode 420 \t avg length: 29 \t reward: -22\n",
      " Episode 440 \t avg length: 29 \t reward: -22\n",
      " Episode 460 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.0206, -0.0121, -0.0060,  ..., -0.0342, -0.0336, -0.0329])\n",
      "Advantages :  tensor([-0.1180, -0.0590,  0.0075,  ...,  3.4982,  3.5671,  3.6417],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4683, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0230, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -23\n",
      " Episode 20 \t avg length: 29 \t reward: -18\n",
      " Episode 40 \t avg length: 29 \t reward: -22\n",
      " Episode 60 \t avg length: 29 \t reward: -31\n",
      "State Values :  tensor([-0.2175, -0.2140, -0.2156,  ..., -0.2208, -0.2171, -0.2191])\n",
      "Advantages :  tensor([0.7027, 0.5958, 0.5991,  ..., 2.7343, 2.6571, 2.6283],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3185, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1910, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -15\n",
      " Episode 100 \t avg length: 29 \t reward: -26\n",
      " Episode 120 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([-0.2175, -0.2140, -0.2156,  ..., -0.2257, -0.2205, -0.2236])\n",
      "Advantages :  tensor([0.7172, 0.5913, 0.5950,  ..., 3.2783, 3.3497, 3.2759],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3132, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1935, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -25\n",
      " Episode 160 \t avg length: 29 \t reward: -24\n",
      " Episode 180 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([-0.2175, -0.2140, -0.2156,  ..., -0.1551, -0.1554, -0.1563])\n",
      "Advantages :  tensor([0.8255, 0.6975, 0.7012,  ..., 3.2337, 3.1053, 3.1776],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1928, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -29\n",
      " Episode 220 \t avg length: 29 \t reward: -33\n",
      " Episode 240 \t avg length: 29 \t reward: -30\n",
      " Episode 260 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([-0.2175, -0.2140, -0.2156,  ..., -0.1381, -0.1352, -0.1355])\n",
      "Advantages :  tensor([1.0188, 0.8950, 0.8986,  ..., 3.2381, 3.2511, 3.2859],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -22\n",
      " Episode 300 \t avg length: 29 \t reward: -27\n",
      " Episode 320 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.2175, -0.2140, -0.2156,  ..., -0.2250, -0.2281, -0.2307])\n",
      "Advantages :  tensor([1.0001, 0.8727, 0.8763,  ..., 3.4640, 3.4729, 3.4768],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3098, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1951, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -22\n",
      " Episode 360 \t avg length: 29 \t reward: -16\n",
      " Episode 380 \t avg length: 29 \t reward: -30\n",
      "State Values :  tensor([-0.2175, -0.2140, -0.2156,  ..., -0.1280, -0.1255, -0.1264])\n",
      "Advantages :  tensor([1.0125, 0.8879, 0.8915,  ..., 3.4181, 3.4163, 3.3125],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1959, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -28\n",
      " Episode 420 \t avg length: 29 \t reward: -22\n",
      " Episode 440 \t avg length: 29 \t reward: -31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 460 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.2175, -0.2140, -0.2156,  ..., -0.2099, -0.2118, -0.2104])\n",
      "Advantages :  tensor([1.0147, 0.8894, 0.8930,  ..., 3.4518, 3.4304, 3.3743],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.3075, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.1964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -22\n",
      " Episode 500 \t avg length: 29 \t reward: -22\n",
      " Episode 20 \t avg length: 29 \t reward: -28\n",
      " Episode 40 \t avg length: 29 \t reward: -25\n",
      " Episode 60 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.2008, 0.1973, 0.1994,  ..., 0.1850, 0.1832, 0.1844])\n",
      "Advantages :  tensor([-0.2955, -0.2624, -0.2254,  ...,  3.5380,  3.6496,  3.6605],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7263, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.2079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -27\n",
      " Episode 100 \t avg length: 29 \t reward: -19\n",
      " Episode 120 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([0.2008, 0.1973, 0.1994,  ..., 0.1418, 0.1371, 0.1383])\n",
      "Advantages :  tensor([-0.4662, -0.4369, -0.4048,  ...,  2.8918,  2.9962,  3.0143],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7147, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.2031, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -23\n",
      " Episode 160 \t avg length: 29 \t reward: -23\n",
      " Episode 180 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([0.2008, 0.1973, 0.1994,  ..., 0.1653, 0.1643, 0.1621])\n",
      "Advantages :  tensor([-0.4636, -0.4339, -0.4014,  ...,  2.6716,  2.8026,  2.9486],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.2024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -22\n",
      " Episode 220 \t avg length: 29 \t reward: -20\n",
      " Episode 240 \t avg length: 29 \t reward: -25\n",
      " Episode 260 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([0.2008, 0.1973, 0.1994,  ..., 0.2033, 0.1977, 0.1976])\n",
      "Advantages :  tensor([-0.5830, -0.5531, -0.5204,  ...,  2.8193,  2.8851,  2.8719],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -17\n",
      " Episode 300 \t avg length: 29 \t reward: -19\n",
      " Episode 320 \t avg length: 29 \t reward: -22\n",
      "State Values :  tensor([0.2008, 0.1973, 0.1994,  ..., 0.2040, 0.1991, 0.1949])\n",
      "Advantages :  tensor([-0.6626, -0.6310, -0.5960,  ...,  2.8497,  2.9522,  3.0607],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7088, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -24\n",
      " Episode 360 \t avg length: 29 \t reward: -22\n",
      " Episode 380 \t avg length: 29 \t reward: -20\n",
      "State Values :  tensor([0.2008, 0.1973, 0.1994,  ..., 0.1351, 0.1366, 0.1348])\n",
      "Advantages :  tensor([-0.7163, -0.6853, -0.6510,  ...,  2.8861,  2.9225,  3.0000],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -18\n",
      " Episode 420 \t avg length: 29 \t reward: -22\n",
      " Episode 440 \t avg length: 29 \t reward: -19\n",
      " Episode 460 \t avg length: 29 \t reward: -27\n",
      "State Values :  tensor([0.2008, 0.1973, 0.1994,  ..., 0.1683, 0.1658, 0.1632])\n",
      "Advantages :  tensor([-0.7087, -0.6778, -0.6437,  ...,  2.8127,  2.8772,  2.9408],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.7073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.1974, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -18\n",
      " Episode 500 \t avg length: 29 \t reward: -25\n",
      "REWARDS:::::: [tensor(0.0799, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.0232, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.1941, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(-0.2013, dtype=torch.float64, grad_fn=<MeanBackward0>)]\n",
      "Sorting Parent Indexes:  [2 0]\n",
      " Data Type:  <class 'numpy.ndarray'>\n",
      "Sorting Completed\n",
      "Selecting Top Parents\n",
      "Generation  2  | Mean rewards:  tensor(0.0240, dtype=torch.float64, grad_fn=<MeanBackward0>)  | Mean of top 5:  tensor(0.1370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Top  2  scores [2 0]\n",
      "Rewards for top:  [tensor(0.1941, dtype=torch.float64, grad_fn=<MeanBackward0>), tensor(0.0799, dtype=torch.float64, grad_fn=<MeanBackward0>)]\n",
      " Episode 20 \t avg length: 29 \t reward: -25\n",
      " Episode 40 \t avg length: 29 \t reward: -31\n",
      " Episode 60 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([0.0454, 0.0463, 0.0470,  ..., 0.0488, 0.0492, 0.0502])\n",
      "Advantages :  tensor([1.1688, 1.2356, 1.2378,  ..., 3.3136, 3.3693, 3.3671],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5180, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0260, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -25\n",
      " Episode 100 \t avg length: 29 \t reward: -33\n",
      " Episode 120 \t avg length: 29 \t reward: -17\n",
      "State Values :  tensor([0.0454, 0.0463, 0.0470,  ..., 0.0433, 0.0397, 0.0363])\n",
      "Advantages :  tensor([1.2524, 1.3119, 1.3137,  ..., 3.1804, 3.1862, 3.1907],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -35\n",
      " Episode 160 \t avg length: 29 \t reward: -28\n",
      " Episode 180 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([ 0.0454,  0.0463,  0.0470,  ..., -0.0239, -0.0289, -0.0309])\n",
      "Advantages :  tensor([1.2250, 1.2841, 1.2860,  ..., 3.1116, 3.1411, 3.2337],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5181, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0270, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -25\n",
      " Episode 220 \t avg length: 29 \t reward: -26\n",
      " Episode 240 \t avg length: 29 \t reward: -25\n",
      " Episode 260 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([ 0.0454,  0.0463,  0.0470,  ..., -0.0082, -0.0118, -0.0155])\n",
      "Advantages :  tensor([1.2028, 1.2635, 1.2655,  ..., 3.1941, 3.2279, 3.2605],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5192, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0273, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -32\n",
      " Episode 300 \t avg length: 29 \t reward: -28\n",
      " Episode 320 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.0454, 0.0463, 0.0470,  ..., 0.0484, 0.0475, 0.0472])\n",
      "Advantages :  tensor([1.3490, 1.4123, 1.4144,  ..., 3.2824, 3.2979, 3.3792],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5181, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -26\n",
      " Episode 360 \t avg length: 29 \t reward: -20\n",
      " Episode 380 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([0.0454, 0.0463, 0.0470,  ..., 0.0575, 0.0584, 0.0591])\n",
      "Advantages :  tensor([1.2727, 1.3364, 1.3385,  ..., 3.1968, 3.3133, 3.3884],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5177, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0274, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -25\n",
      " Episode 420 \t avg length: 29 \t reward: -24\n",
      " Episode 440 \t avg length: 29 \t reward: -22\n",
      " Episode 460 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([0.0454, 0.0463, 0.0470,  ..., 0.0525, 0.0539, 0.0553])\n",
      "Advantages :  tensor([1.1312, 1.1923, 1.1942,  ..., 2.9890, 3.1012, 3.2542],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.5182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(-0.0274, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -26\n",
      " Episode 500 \t avg length: 29 \t reward: -35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for elite i  2  is  tensor(-0.0270, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 20 \t avg length: 29 \t reward: -17\n",
      " Episode 40 \t avg length: 29 \t reward: -22\n",
      " Episode 60 \t avg length: 29 \t reward: -19\n",
      "State Values :  tensor([-0.0187, -0.0232, -0.0272,  ..., -0.0530, -0.0520, -0.0520])\n",
      "Advantages :  tensor([0.8107, 0.8481, 0.8963,  ..., 3.1529, 3.1141, 3.1122],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4762, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0111, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -20\n",
      " Episode 100 \t avg length: 29 \t reward: -25\n",
      " Episode 120 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0187, -0.0232, -0.0272,  ...,  0.0195,  0.0162,  0.0163])\n",
      "Advantages :  tensor([1.2524, 1.2903, 1.3392,  ..., 3.3058, 3.4212, 3.4941],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4756, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -27\n",
      " Episode 160 \t avg length: 29 \t reward: -21\n",
      " Episode 180 \t avg length: 29 \t reward: -34\n",
      "State Values :  tensor([-0.0187, -0.0232, -0.0272,  ..., -0.0197, -0.0205, -0.0239])\n",
      "Advantages :  tensor([1.1909, 1.2222, 1.2623,  ..., 2.9384, 2.9937, 2.9889],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4782, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0090, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -15\n",
      " Episode 220 \t avg length: 29 \t reward: -23\n",
      " Episode 240 \t avg length: 29 \t reward: -21\n",
      " Episode 260 \t avg length: 29 \t reward: -33\n",
      "State Values :  tensor([-0.0187, -0.0232, -0.0272,  ..., -0.0064, -0.0061, -0.0097])\n",
      "Advantages :  tensor([1.3569, 1.3893, 1.4309,  ..., 3.1049, 3.1714, 3.2682],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4781, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -23\n",
      " Episode 300 \t avg length: 29 \t reward: -29\n",
      " Episode 320 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([-0.0187, -0.0232, -0.0272,  ...,  0.0097,  0.0135,  0.0102])\n",
      "Advantages :  tensor([1.3638, 1.3954, 1.4359,  ..., 2.9755, 3.0520, 3.1532],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4781, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0088, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -19\n",
      " Episode 360 \t avg length: 29 \t reward: -23\n",
      " Episode 380 \t avg length: 29 \t reward: -16\n",
      "State Values :  tensor([-0.0187, -0.0232, -0.0272,  ..., -0.0067, -0.0064, -0.0031])\n",
      "Advantages :  tensor([1.3075, 1.3390, 1.3794,  ..., 3.1123, 3.1331, 3.1600],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4780, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0084, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -25\n",
      " Episode 420 \t avg length: 29 \t reward: -23\n",
      " Episode 440 \t avg length: 29 \t reward: -18\n",
      " Episode 460 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([-0.0187, -0.0232, -0.0272,  ...,  0.0300,  0.0311,  0.0305])\n",
      "Advantages :  tensor([1.2756, 1.3072, 1.3477,  ..., 3.0922, 3.1570, 3.1168],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -20\n",
      " Episode 500 \t avg length: 29 \t reward: -24\n",
      "Score for elite i  0  is  tensor(0.0092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 20 \t avg length: 29 \t reward: -20\n",
      " Episode 40 \t avg length: 29 \t reward: -28\n",
      " Episode 60 \t avg length: 29 \t reward: -18\n",
      "State Values :  tensor([ 0.0019,  0.0029,  0.0035,  ..., -0.0026, -0.0024, -0.0041])\n",
      "Advantages :  tensor([0.3488, 0.4062, 0.3815,  ..., 2.4169, 2.5327, 2.6180],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4560, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 80 \t avg length: 29 \t reward: -28\n",
      " Episode 100 \t avg length: 29 \t reward: -27\n",
      " Episode 120 \t avg length: 29 \t reward: -26\n",
      "State Values :  tensor([ 0.0019,  0.0029,  0.0035,  ..., -0.0133, -0.0164, -0.0177])\n",
      "Advantages :  tensor([0.7154, 0.7715, 0.7473,  ..., 3.1536, 3.0912, 3.0632],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4587, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0307, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 140 \t avg length: 29 \t reward: -27\n",
      " Episode 160 \t avg length: 29 \t reward: -25\n",
      " Episode 180 \t avg length: 29 \t reward: -31\n",
      "State Values :  tensor([ 0.0019,  0.0029,  0.0035,  ..., -0.0192, -0.0207, -0.0204])\n",
      "Advantages :  tensor([0.7512, 0.8091, 0.7842,  ..., 3.0313, 3.1126, 3.1494],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4609, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0301, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 200 \t avg length: 29 \t reward: -19\n",
      " Episode 220 \t avg length: 29 \t reward: -25\n",
      " Episode 240 \t avg length: 29 \t reward: -24\n",
      " Episode 260 \t avg length: 29 \t reward: -21\n",
      "State Values :  tensor([ 0.0019,  0.0029,  0.0035,  ..., -0.0406, -0.0396, -0.0388])\n",
      "Advantages :  tensor([0.7523, 0.8138, 0.7874,  ..., 3.4142, 3.3957, 3.3780],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4611, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0300, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 280 \t avg length: 29 \t reward: -30\n",
      " Episode 300 \t avg length: 29 \t reward: -23\n",
      " Episode 320 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([ 0.0019,  0.0029,  0.0035,  ..., -0.0374, -0.0380, -0.0379])\n",
      "Advantages :  tensor([0.7853, 0.8474, 0.8208,  ..., 3.3244, 3.3862, 3.3540],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 340 \t avg length: 29 \t reward: -27\n",
      " Episode 360 \t avg length: 29 \t reward: -20\n",
      " Episode 380 \t avg length: 29 \t reward: -24\n",
      "State Values :  tensor([ 0.0019,  0.0029,  0.0035,  ..., -0.0199, -0.0213, -0.0225])\n",
      "Advantages :  tensor([0.7954, 0.8605, 0.8327,  ..., 3.4559, 3.4656, 3.4807],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4610, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0301, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 400 \t avg length: 29 \t reward: -23\n",
      " Episode 420 \t avg length: 29 \t reward: -31\n",
      " Episode 440 \t avg length: 29 \t reward: -27\n",
      " Episode 460 \t avg length: 29 \t reward: -25\n",
      "State Values :  tensor([ 0.0019,  0.0029,  0.0035,  ..., -0.0455, -0.0457, -0.0475])\n",
      "Advantages :  tensor([0.8992, 0.9664, 0.9377,  ..., 3.4899, 3.5396, 3.6501],\n",
      "       dtype=torch.float64)\n",
      "L Clip :  tensor(0.4615, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "returned Surrogate Loss  tensor(0.0299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      " Episode 480 \t avg length: 29 \t reward: -25\n",
      " Episode 500 \t avg length: 29 \t reward: -25\n",
      "Score for elite i  3  is  tensor(0.0304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Elite selected with index  3  and score tensor(0.0304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "game_actions = 2 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "#torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 4\n",
    "agents = return_random_agents(num_agents)\n",
    "\n",
    "# How many top agents to consider as parents\n",
    "top_limit = 2\n",
    "\n",
    "# run evolution until X generations\n",
    "generations = 3\n",
    "\n",
    "elite_index = None\n",
    "\n",
    "for generation in range(generations):\n",
    "\n",
    "    # return rewards of agents\n",
    "    rewards = run_agents_n_times(agents, 1) #return average of 3 runs\n",
    "    print(\"REWARDS::::::\",rewards)\n",
    "    # sort by rewards\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]#reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "    print(\"Sorting Parent Indexes: \",sorted_parent_indexes)\n",
    "    print(\" Data Type: \", type(sorted_parent_indexes))\n",
    "    print(\"Sorting Completed\")\n",
    "    print(\"Selecting Top Parents\")\n",
    "    \n",
    "    top_rewards = []\n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "    print(\"Generation \", generation, \" | Mean rewards: \", torch.mean(torch.stack(rewards)), \" | Mean of top 5: \",torch.mean(torch.stack(top_rewards[:5])))\n",
    "    #print(rewards)\n",
    "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    \n",
    "    # setup an empty list for containing children agents\n",
    "    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "\n",
    "    # kill all agents, and replace them with their children\n",
    "    agents = children_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        \n",
    "        env_record = Monitor(env, './video', force=True)\n",
    "        observation = env_record.reset()\n",
    "        last_observation = observation\n",
    "        r=0\n",
    "        for _ in range(250):\n",
    "            env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env_record.step(action)\n",
    "            r=r+reward\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        env_record.close()\n",
    "        print(\"Rewards: \",r)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' and 'p' must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-5e4c93bb9c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-b8ca255d9fc6>\u001b[0m in \u001b[0;36mplay_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' and 'p' must have same size"
     ]
    }
   ],
   "source": [
    "play_agent(agents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
