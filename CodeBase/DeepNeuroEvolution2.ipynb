{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAI(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                        nn.Linear(4,128, bias=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128,2, bias=True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )\n",
    "\n",
    "                \n",
    "        def forward(self, inputs):\n",
    "            x = self.fc(inputs)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \n",
    "        # nn.Conv2d weights are of shape [16, 1, 3, 3] i.e. # number of filters, 1, stride, stride\n",
    "        # nn.Conv2d bias is of shape [16] i.e. # number of filters\n",
    "        \n",
    "        # nn.Linear weights are of shape [32, 24336] i.e. # number of input features, number of output features\n",
    "        # nn.Linear bias is of shape [32] i.e. # number of output features\n",
    "        \n",
    "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_agents(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        \n",
    "        agent = CartPoleAI()\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "        \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "    \n",
    "        observation = env.reset()\n",
    "        \n",
    "        r=0\n",
    "        s=0\n",
    "        \n",
    "        for _ in range(250):\n",
    "            \n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            r=r+reward\n",
    "            \n",
    "            s=s+1\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        reward_agents.append(r)        \n",
    "        #reward_agents.append(s)\n",
    "        \n",
    "    \n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_agents([agent])[0]\n",
    "    return score/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    avg_score = []\n",
    "    for agent in agents:\n",
    "        avg_score.append(return_average_score(agent,runs))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    \n",
    "    mutation_power = 0.02 #hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "    \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            \n",
    "                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                                \n",
    "                                    \n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    \n",
    "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
    "    for i in range(len(agents)-1):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    #now add one elite\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index=len(children_agents)-1 #it is the last one\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    \n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if(elite_index is not None):\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation  0  | Mean rewards:  21.518666666666665  | Mean of top 5:  43.33333333333333\n",
      "Top  20  scores [236 332 134 266 157 438  91 235 133  39 419  98 219 124 211 190 107  69\n",
      "  67 344]\n",
      "Rewards for top:  [47.333333333333336, 45.666666666666664, 42.666666666666664, 40.666666666666664, 40.333333333333336, 39.333333333333336, 39.333333333333336, 38.666666666666664, 38.0, 38.0, 37.666666666666664, 37.666666666666664, 37.666666666666664, 37.333333333333336, 37.333333333333336, 36.666666666666664, 36.666666666666664, 35.666666666666664, 35.333333333333336, 35.333333333333336]\n",
      "Score for elite i  236  is  28.4\n",
      "Score for elite i  332  is  17.8\n",
      "Score for elite i  134  is  18.6\n",
      "Score for elite i  266  is  12.4\n",
      "Score for elite i  157  is  28.0\n",
      "Score for elite i  438  is  21.2\n",
      "Score for elite i  91  is  32.8\n",
      "Score for elite i  235  is  17.2\n",
      "Score for elite i  133  is  18.4\n",
      "Score for elite i  39  is  14.4\n",
      "Elite selected with index  91  and score 32.8\n",
      "\n",
      "\n",
      "Generation  1  | Mean rewards:  22.46066666666667  | Mean of top 5:  48.93333333333333\n",
      "Top  20  scores [301 236 101 471 491 131 425  98 433  94 300 340 357 126 451 305 388 141\n",
      " 332 261]\n",
      "Rewards for top:  [51.333333333333336, 50.666666666666664, 48.0, 47.666666666666664, 47.0, 46.333333333333336, 44.666666666666664, 44.0, 43.666666666666664, 43.0, 42.333333333333336, 42.0, 40.0, 39.333333333333336, 39.333333333333336, 39.0, 38.666666666666664, 38.333333333333336, 38.333333333333336, 37.666666666666664]\n",
      "Score for elite i  301  is  22.0\n",
      "Score for elite i  236  is  20.2\n",
      "Score for elite i  101  is  19.4\n",
      "Score for elite i  471  is  14.4\n",
      "Score for elite i  491  is  18.2\n",
      "Score for elite i  131  is  30.4\n",
      "Score for elite i  425  is  23.8\n",
      "Score for elite i  98  is  25.4\n",
      "Score for elite i  433  is  26.6\n",
      "Score for elite i  94  is  15.8\n",
      "Score for elite i  499  is  23.0\n",
      "Elite selected with index  131  and score 30.4\n",
      "\n",
      "\n",
      "Generation  2  | Mean rewards:  23.08066666666667  | Mean of top 5:  56.06666666666666\n",
      "Top  20  scores [115 297 498 200  50  76 389 174 109 127 318 304 258  23 251  68 319 207\n",
      " 321 232]\n",
      "Rewards for top:  [66.33333333333333, 56.333333333333336, 54.0, 53.0, 50.666666666666664, 48.666666666666664, 45.666666666666664, 45.0, 44.333333333333336, 43.666666666666664, 42.666666666666664, 42.666666666666664, 42.0, 42.0, 41.666666666666664, 41.333333333333336, 40.666666666666664, 40.333333333333336, 40.333333333333336, 40.0]\n",
      "Score for elite i  115  is  22.8\n",
      "Score for elite i  297  is  27.0\n",
      "Score for elite i  498  is  19.0\n",
      "Score for elite i  200  is  16.2\n",
      "Score for elite i  50  is  28.6\n",
      "Score for elite i  76  is  17.6\n",
      "Score for elite i  389  is  20.2\n",
      "Score for elite i  174  is  30.4\n",
      "Score for elite i  109  is  17.2\n",
      "Score for elite i  127  is  36.0\n",
      "Score for elite i  499  is  29.8\n",
      "Elite selected with index  127  and score 36.0\n",
      "\n",
      "\n",
      "Generation  3  | Mean rewards:  23.706666666666667  | Mean of top 5:  50.93333333333332\n",
      "Top  20  scores [419 190  75 436 164  69   0 291  98  78 421 183 128 349  24 206  62 335\n",
      " 358 497]\n",
      "Rewards for top:  [53.0, 51.666666666666664, 51.666666666666664, 49.666666666666664, 48.666666666666664, 48.333333333333336, 48.0, 48.0, 47.666666666666664, 47.666666666666664, 45.333333333333336, 45.0, 44.333333333333336, 44.333333333333336, 44.0, 42.666666666666664, 42.666666666666664, 42.666666666666664, 42.0, 41.666666666666664]\n",
      "Score for elite i  419  is  25.0\n",
      "Score for elite i  190  is  19.2\n",
      "Score for elite i  75  is  18.6\n",
      "Score for elite i  436  is  16.2\n",
      "Score for elite i  164  is  25.4\n",
      "Score for elite i  69  is  27.2\n",
      "Score for elite i  0  is  28.0\n",
      "Score for elite i  291  is  25.0\n",
      "Score for elite i  98  is  32.8\n",
      "Score for elite i  78  is  22.4\n",
      "Score for elite i  499  is  21.8\n",
      "Elite selected with index  98  and score 32.8\n",
      "\n",
      "\n",
      "Generation  4  | Mean rewards:  24.17666666666667  | Mean of top 5:  57.733333333333334\n",
      "Top  20  scores [194 190  75  15  99 107 357  40 433 245 420  29  65 221 199 124 328  63\n",
      " 394 368]\n",
      "Rewards for top:  [65.0, 58.0, 56.333333333333336, 56.333333333333336, 53.0, 53.0, 51.0, 50.333333333333336, 49.333333333333336, 48.666666666666664, 48.666666666666664, 45.666666666666664, 44.0, 44.0, 43.333333333333336, 43.0, 43.0, 42.333333333333336, 41.666666666666664, 41.666666666666664]\n",
      "Score for elite i  194  is  43.6\n",
      "Score for elite i  190  is  20.0\n",
      "Score for elite i  75  is  18.4\n",
      "Score for elite i  15  is  29.0\n",
      "Score for elite i  99  is  21.8\n",
      "Score for elite i  107  is  24.6\n",
      "Score for elite i  357  is  23.4\n",
      "Score for elite i  40  is  17.8\n",
      "Score for elite i  433  is  22.2\n",
      "Score for elite i  245  is  27.8\n",
      "Score for elite i  499  is  42.4\n",
      "Elite selected with index  194  and score 43.6\n",
      "\n",
      "\n",
      "Generation  5  | Mean rewards:  25.606666666666666  | Mean of top 5:  54.733333333333334\n",
      "Top  20  scores [233 322 268 339  67  51 341 463  52 218 401 275 296 355 442   1  80 170\n",
      " 455 434]\n",
      "Rewards for top:  [68.33333333333333, 54.666666666666664, 52.333333333333336, 49.666666666666664, 48.666666666666664, 48.333333333333336, 48.333333333333336, 48.0, 47.333333333333336, 47.0, 46.666666666666664, 46.666666666666664, 45.0, 45.0, 44.666666666666664, 44.666666666666664, 44.0, 43.333333333333336, 43.333333333333336, 43.0]\n",
      "Score for elite i  233  is  34.4\n",
      "Score for elite i  322  is  19.2\n",
      "Score for elite i  268  is  26.6\n",
      "Score for elite i  339  is  34.2\n",
      "Score for elite i  67  is  19.6\n",
      "Score for elite i  51  is  18.6\n",
      "Score for elite i  341  is  26.4\n",
      "Score for elite i  463  is  21.0\n",
      "Score for elite i  52  is  21.8\n",
      "Score for elite i  218  is  20.0\n",
      "Score for elite i  499  is  25.0\n",
      "Elite selected with index  233  and score 34.4\n",
      "\n",
      "\n",
      "Generation  6  | Mean rewards:  27.270666666666664  | Mean of top 5:  67.00000000000001\n",
      "Top  20  scores [ 97 379 291 202  38   7 326 427 214 208 405 140 193  31  14 289 282 257\n",
      " 261 316]\n",
      "Rewards for top:  [84.66666666666667, 68.66666666666667, 66.0, 59.0, 56.666666666666664, 53.333333333333336, 53.0, 53.0, 52.333333333333336, 52.333333333333336, 51.0, 50.666666666666664, 50.0, 49.666666666666664, 48.0, 47.333333333333336, 47.0, 47.0, 45.666666666666664, 45.333333333333336]\n",
      "Score for elite i  97  is  17.6\n",
      "Score for elite i  379  is  23.2\n",
      "Score for elite i  291  is  26.4\n",
      "Score for elite i  202  is  39.2\n",
      "Score for elite i  38  is  20.8\n",
      "Score for elite i  7  is  28.2\n",
      "Score for elite i  326  is  21.6\n",
      "Score for elite i  427  is  26.4\n",
      "Score for elite i  214  is  24.0\n",
      "Score for elite i  208  is  22.4\n",
      "Score for elite i  499  is  32.0\n",
      "Elite selected with index  202  and score 39.2\n",
      "\n",
      "\n",
      "Generation  7  | Mean rewards:  27.752  | Mean of top 5:  65.26666666666668\n",
      "Top  20  scores [433 305 368  87 310  83 295 242 304 106 314 387 162 489 281  19 426 313\n",
      "  13 365]\n",
      "Rewards for top:  [78.0, 65.0, 62.333333333333336, 62.0, 59.0, 59.0, 57.0, 55.333333333333336, 53.666666666666664, 53.333333333333336, 53.0, 52.0, 51.333333333333336, 51.333333333333336, 49.666666666666664, 49.0, 48.666666666666664, 48.666666666666664, 48.0, 47.666666666666664]\n",
      "Score for elite i  433  is  57.8\n",
      "Score for elite i  305  is  23.4\n",
      "Score for elite i  368  is  35.0\n",
      "Score for elite i  87  is  29.8\n",
      "Score for elite i  310  is  32.6\n",
      "Score for elite i  83  is  17.0\n",
      "Score for elite i  295  is  20.0\n",
      "Score for elite i  242  is  37.4\n",
      "Score for elite i  304  is  20.0\n",
      "Score for elite i  106  is  18.4\n",
      "Score for elite i  499  is  20.8\n",
      "Elite selected with index  433  and score 57.8\n",
      "\n",
      "\n",
      "Generation  8  | Mean rewards:  27.878  | Mean of top 5:  58.53333333333334\n",
      "Top  20  scores [263 306 394  85  55 359  78 159 439 455 228 266  75 390 191 413 165 157\n",
      " 236 217]\n",
      "Rewards for top:  [66.66666666666667, 57.333333333333336, 57.333333333333336, 56.0, 55.333333333333336, 55.0, 53.666666666666664, 53.333333333333336, 52.333333333333336, 50.333333333333336, 50.333333333333336, 50.0, 49.333333333333336, 49.0, 48.666666666666664, 48.333333333333336, 48.0, 48.0, 47.666666666666664, 47.0]\n",
      "Score for elite i  263  is  28.8\n",
      "Score for elite i  306  is  32.4\n",
      "Score for elite i  394  is  46.6\n",
      "Score for elite i  85  is  26.4\n",
      "Score for elite i  55  is  33.2\n",
      "Score for elite i  359  is  32.4\n",
      "Score for elite i  78  is  26.8\n",
      "Score for elite i  159  is  21.4\n",
      "Score for elite i  439  is  22.0\n",
      "Score for elite i  455  is  16.8\n",
      "Score for elite i  499  is  29.2\n",
      "Elite selected with index  394  and score 46.6\n"
     ]
    }
   ],
   "source": [
    "game_actions = 2 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 500\n",
    "agents = return_random_agents(num_agents)\n",
    "\n",
    "# How many top agents to consider as parents\n",
    "top_limit = 20\n",
    "\n",
    "# run evolution until X generations\n",
    "generations = 10\n",
    "\n",
    "elite_index = None\n",
    "n=[]\n",
    "\n",
    "for generation in range(generations):\n",
    "\n",
    "    # return rewards of agents\n",
    "    rewards = run_agents_n_times(agents, 3) #return average of 3 runs\n",
    "\n",
    "    # sort by rewards\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit] #reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    top_rewards = []\n",
    "    \n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "    print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "    #print(rewards)\n",
    "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    \n",
    "    \n",
    "    n.append(np.mean(rewards))\n",
    "    # setup an empty list for containing children agents\n",
    "    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "\n",
    "    # kill all agents, and replace them with their children\n",
    "    agents = children_agents\n",
    "x=np.arange(generations)\n",
    "plt.plot(x,n)\n",
    "plt.title('Improvement of Mean Rewards in increasing Generations(Training)')\n",
    "plt.ylabel('Mean Rewards')\n",
    "plt.xlabel('Generations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        \n",
    "        env_record = Monitor(env, './video', force=True)\n",
    "        observation = env_record.reset()\n",
    "        last_observation = observation\n",
    "        r=0\n",
    "        j=[]\n",
    "        episode_durations=[]\n",
    "        timestep=0\n",
    "        for timestep in range(250):\n",
    "            env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env_record.step(action)\n",
    "            r=r+reward\n",
    "            j.append(r)\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        env_record.close()\n",
    "\n",
    "        print(\"Rewards: \",r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_agent(agents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
